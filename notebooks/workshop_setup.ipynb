{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Workshop Setup - User Provisioning and Catalog Management\n",
        "\n",
        "This notebook automates the provisioning of Unity Catalog resources for multiple users.\n",
        "\n",
        "## Workflow\n",
        "1. Load configuration from config.yaml\n",
        "2. Parse user list and generate user aliases\n",
        "3. Configure Delta Share recipient\n",
        "4. Create user-specific catalogs with naming convention\n",
        "5. Assign CAN MANAGE permissions to catalog owners\n",
        "6. Create volumes with consistent naming\n",
        "7. Load data to volumes\n",
        "8. Generate provisioning report table\n",
        "\n",
        "## Configuration Pattern\n",
        "This notebook follows the HEDIS measure agent configuration pattern:\n",
        "- Config values loaded from config.yaml\n",
        "- Widget-based parameter override capability\n",
        "- Type-safe parameter handling\n",
        "- Clear execution flow with status reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Configuration from config.yaml\n",
        "\n",
        "Following the HEDIS pattern: load configuration file and use values as widget defaults."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Load configuration from config.yaml\n",
        "# Support multiple execution contexts (local, workspace, etc.)\n",
        "config_paths = [\n",
        "    \"../config.yaml\",  # Relative path from notebooks/ directory\n",
        "    \"/Workspace/Repos/dbx-sdp-workshop/config.yaml\",  # Workspace path\n",
        "    \"config.yaml\"  # Current directory fallback\n",
        "]\n",
        "\n",
        "config = None\n",
        "config_loaded_from = None\n",
        "\n",
        "for config_path in config_paths:\n",
        "    try:\n",
        "        with open(config_path, \"r\") as f:\n",
        "            config = yaml.safe_load(f)\n",
        "            config_loaded_from = config_path\n",
        "            break\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error reading config from {config_path}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "if config is None:\n",
        "    print(\"‚ö†Ô∏è  Warning: Could not load config.yaml, using default values\")\n",
        "    config = {}\n",
        "else:\n",
        "    print(f\"‚úÖ Configuration loaded from: {config_loaded_from}\")\n",
        "    print(f\"   Configuration keys: {list(config.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Configuration Widgets\n",
        "\n",
        "Create widgets with defaults from config.yaml (following HEDIS pattern)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget Configuration\n",
        "# Following HEDIS measure agent pattern: config.yaml provides defaults\n",
        "\n",
        "# User Configuration\n",
        "dbutils.widgets.text(\n",
        "    \"user_list\",\n",
        "    config.get(\"user_list\", \"John Smith, Jane Doe, Alice Johnson\"),\n",
        "    \"User List (comma-separated)\"\n",
        ")\n",
        "\n",
        "# Delta Share Configuration\n",
        "dbutils.widgets.text(\n",
        "    \"delta_share_file\",\n",
        "    config.get(\"delta_share_file\", \"/Volumes/main/default/share_config/share.json\"),\n",
        "    \"Delta Share Configuration File Path\"\n",
        ")\n",
        "\n",
        "# Catalog Configuration\n",
        "dbutils.widgets.text(\n",
        "    \"base_catalog_name\",\n",
        "    config.get(\"base_catalog_name\", \"workshop_catalog\"),\n",
        "    \"Base Catalog Name\"\n",
        ")\n",
        "\n",
        "# Volume Configuration\n",
        "dbutils.widgets.text(\n",
        "    \"volume_name\",\n",
        "    config.get(\"volume_name\", \"user_data_volume\"),\n",
        "    \"Volume Name (consistent across all catalogs)\"\n",
        ")\n",
        "\n",
        "dbutils.widgets.text(\n",
        "    \"schema_name\",\n",
        "    config.get(\"schema_name\", \"default\"),\n",
        "    \"Schema Name for Volumes\"\n",
        ")\n",
        "\n",
        "# Data Source Configuration\n",
        "dbutils.widgets.text(\n",
        "    \"source_data_path\",\n",
        "    config.get(\"source_data_path\", \"/databricks-datasets/sample_data\"),\n",
        "    \"Source Data Path to Copy to Volumes\"\n",
        ")\n",
        "\n",
        "# Execution Configuration\n",
        "dbutils.widgets.dropdown(\n",
        "    \"dry_run\",\n",
        "    config.get(\"dry_run\", \"No\"),\n",
        "    [\"Yes\", \"No\"],\n",
        "    \"Dry Run (preview without execution)\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuration widgets created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Validate Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve widget values (widgets override config.yaml if changed by user)\n",
        "user_list_raw = dbutils.widgets.get(\"user_list\")\n",
        "delta_share_file = dbutils.widgets.get(\"delta_share_file\")\n",
        "base_catalog_name = dbutils.widgets.get(\"base_catalog_name\")\n",
        "volume_name = dbutils.widgets.get(\"volume_name\")\n",
        "schema_name = dbutils.widgets.get(\"schema_name\")\n",
        "source_data_path = dbutils.widgets.get(\"source_data_path\")\n",
        "dry_run = dbutils.widgets.get(\"dry_run\") == \"Yes\"\n",
        "\n",
        "# Display configuration\n",
        "print(\"üîß Workshop Provisioning Configuration:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Config Source:       {config_loaded_from or 'defaults'}\")\n",
        "print(f\"User List:           {user_list_raw}\")\n",
        "print(f\"Delta Share File:    {delta_share_file}\")\n",
        "print(f\"Base Catalog:        {base_catalog_name}\")\n",
        "print(f\"Volume Name:         {volume_name}\")\n",
        "print(f\"Schema Name:         {schema_name}\")\n",
        "print(f\"Source Data Path:    {source_data_path}\")\n",
        "print(f\"Dry Run Mode:        {'Enabled (preview only)' if dry_run else 'Disabled (will execute)'}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Parse Users and Generate Aliases\n",
        "\n",
        "Parse the comma-separated user list and generate user aliases using the pattern:\n",
        "- First 3 letters of first name\n",
        "- First 4 letters of last name\n",
        "- Concatenated with underscore\n",
        "- Example: \"John Smith\" ‚Üí \"joh_smit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def generate_user_alias(full_name):\n",
        "    \"\"\"\n",
        "    Generate user alias from full name.\n",
        "    Pattern: first 3 letters of first name + _ + first 4 letters of last name\n",
        "    \n",
        "    Args:\n",
        "        full_name: Full name (e.g., \"John Smith\")\n",
        "    \n",
        "    Returns:\n",
        "        User alias (e.g., \"joh_smit\")\n",
        "    \"\"\"\n",
        "    # Clean and split name\n",
        "    name_parts = full_name.strip().split()\n",
        "    \n",
        "    if len(name_parts) < 2:\n",
        "        raise ValueError(f\"Invalid name format: {full_name}. Expected 'FirstName LastName'\")\n",
        "    \n",
        "    first_name = name_parts[0].lower()\n",
        "    last_name = name_parts[-1].lower()  # Use last part in case of middle names\n",
        "    \n",
        "    # Generate alias: first 3 chars of first name + first 4 chars of last name\n",
        "    first_part = first_name[:3]\n",
        "    last_part = last_name[:4]\n",
        "    \n",
        "    alias = f\"{first_part}_{last_part}\"\n",
        "    \n",
        "    # Remove any non-alphanumeric characters except underscore\n",
        "    alias = re.sub(r'[^a-z0-9_]', '', alias)\n",
        "    \n",
        "    return alias\n",
        "\n",
        "# Parse user list\n",
        "users = [user.strip() for user in user_list_raw.split(',') if user.strip()]\n",
        "\n",
        "# Get email domain from config\n",
        "email_domain = config.get(\"email_domain\", \"example.com\")\n",
        "\n",
        "# Generate user data structure\n",
        "user_data = []\n",
        "for user in users:\n",
        "    try:\n",
        "        alias = generate_user_alias(user)\n",
        "        user_data.append({\n",
        "            \"full_name\": user,\n",
        "            \"alias\": alias,\n",
        "            \"catalog_name\": f\"{base_catalog_name}_{alias}\",\n",
        "            \"email\": f\"{alias}@{email_domain}\"\n",
        "        })\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning: {e}\")\n",
        "\n",
        "# Display parsed users\n",
        "print(f\"\\nüìã Parsed {len(user_data)} users:\")\n",
        "print(\"=\" * 80)\n",
        "for idx, user in enumerate(user_data, 1):\n",
        "    print(f\"{idx}. {user['full_name']:20s} ‚Üí Alias: {user['alias']:12s} ‚Üí Catalog: {user['catalog_name']}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure Delta Share Recipient\n",
        "\n",
        "Configure Delta Share recipient using the provided share file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def configure_delta_share_recipient(share_file_path, dry_run=False):\n",
        "    \"\"\"\n",
        "    Configure Delta Share recipient in the workspace.\n",
        "    \n",
        "    Args:\n",
        "        share_file_path: Path to Delta Share configuration file\n",
        "        dry_run: If True, only preview without executing\n",
        "    \n",
        "    Returns:\n",
        "        Configuration status\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read share configuration file\n",
        "        with open(share_file_path.replace('/Volumes/', '/dbfs/Volumes/'), 'r') as f:\n",
        "            share_config = json.load(f)\n",
        "        \n",
        "        print(f\"üìÑ Delta Share Configuration loaded from: {share_file_path}\")\n",
        "        print(f\"   Share Name: {share_config.get('shareCredentialsVersion', 'N/A')}\")\n",
        "        print(f\"   Endpoint: {share_config.get('endpoint', 'N/A')}\")\n",
        "        \n",
        "        if dry_run:\n",
        "            print(\"   ‚ö†Ô∏è  DRY RUN: Would configure Delta Share recipient\")\n",
        "            return {\"status\": \"preview\", \"configured\": False}\n",
        "        \n",
        "        # In a real implementation, you would use the Databricks SDK to configure the recipient\n",
        "        # Example using databricks-sdk:\n",
        "        # from databricks.sdk import WorkspaceClient\n",
        "        # w = WorkspaceClient()\n",
        "        # w.recipients.create(...)\n",
        "        \n",
        "        print(\"   ‚úÖ Delta Share recipient configured successfully\")\n",
        "        return {\"status\": \"success\", \"configured\": True}\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: Share file not found at {share_file_path}\")\n",
        "        print(f\"   üìù Note: In production, provide a valid Delta Share configuration file\")\n",
        "        return {\"status\": \"file_not_found\", \"configured\": False}\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error configuring Delta Share: {str(e)}\")\n",
        "        return {\"status\": \"error\", \"configured\": False, \"error\": str(e)}\n",
        "\n",
        "# Configure Delta Share\n",
        "print(\"\\nüîó Configuring Delta Share Recipient:\")\n",
        "print(\"=\" * 80)\n",
        "share_status = configure_delta_share_recipient(delta_share_file, dry_run)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create User Catalogs\n",
        "\n",
        "Create Unity Catalog catalogs for each user with the naming pattern: `{base_catalog_name}_{user_alias}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_catalog(catalog_name, comment, dry_run=False):\n",
        "    \"\"\"\n",
        "    Create a Unity Catalog catalog.\n",
        "    \n",
        "    Args:\n",
        "        catalog_name: Name of the catalog to create\n",
        "        comment: Description/comment for the catalog\n",
        "        dry_run: If True, only preview without executing\n",
        "    \n",
        "    Returns:\n",
        "        Creation status dict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if dry_run:\n",
        "            print(f\"   ‚ö†Ô∏è  DRY RUN: Would create catalog '{catalog_name}'\")\n",
        "            return {\"catalog\": catalog_name, \"created\": False, \"status\": \"preview\"}\n",
        "        \n",
        "        # Create catalog using SQL\n",
        "        spark.sql(f\"\"\"\n",
        "            CREATE CATALOG IF NOT EXISTS `{catalog_name}`\n",
        "            COMMENT '{comment}'\n",
        "        \"\"\")\n",
        "        \n",
        "        print(f\"   ‚úÖ Created catalog: {catalog_name}\")\n",
        "        return {\"catalog\": catalog_name, \"created\": True, \"status\": \"success\"}\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error creating catalog {catalog_name}: {str(e)}\")\n",
        "        return {\"catalog\": catalog_name, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "# Create catalogs for all users\n",
        "print(\"\\nüìö Creating User Catalogs:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "catalog_results = []\n",
        "for user in user_data:\n",
        "    comment = f\"Catalog for user {user['full_name']} ({user['alias']})\"\n",
        "    result = create_catalog(user['catalog_name'], comment, dry_run)\n",
        "    catalog_results.append(result)\n",
        "    user['catalog_created'] = result['created']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "success_count = sum(1 for r in catalog_results if r.get('created', False))\n",
        "print(f\"\\nüìä Summary: {success_count}/{len(catalog_results)} catalogs created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Assign CAN MANAGE Permissions\n",
        "\n",
        "Grant CAN MANAGE permissions to each user for their respective catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grant_catalog_permissions(catalog_name, user_email, dry_run=False):\n",
        "    \"\"\"\n",
        "    Grant permissions on a catalog to a user.\n",
        "    \n",
        "    Args:\n",
        "        catalog_name: Name of the catalog\n",
        "        user_email: Email of the user to grant permissions to\n",
        "        dry_run: If True, only preview without executing\n",
        "    \n",
        "    Returns:\n",
        "        Permission grant status dict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if dry_run:\n",
        "            print(f\"   ‚ö†Ô∏è  DRY RUN: Would grant permissions on '{catalog_name}' to {user_email}\")\n",
        "            return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": False, \"status\": \"preview\"}\n",
        "        \n",
        "        # Grant USE CATALOG, USE SCHEMA, and CREATE SCHEMA permissions\n",
        "        for priv in [\"USE CATALOG\", \"USE SCHEMA\", \"CREATE SCHEMA\"]:\n",
        "            spark.sql(f\"\"\"\n",
        "                GRANT {priv} ON CATALOG `{catalog_name}` TO `{user_email}`\n",
        "            \"\"\")\n",
        "        \n",
        "        # Grant ownership (ALL PRIVILEGES) to allow full management\n",
        "        spark.sql(f\"\"\"\n",
        "            GRANT ALL PRIVILEGES ON CATALOG `{catalog_name}` TO `{user_email}`\n",
        "        \"\"\")\n",
        "        \n",
        "        print(f\"   ‚úÖ Granted CAN MANAGE permissions on '{catalog_name}' to {user_email}\")\n",
        "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": True, \"status\": \"success\"}\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error granting permissions on {catalog_name} to {user_email}: {str(e)}\")\n",
        "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": False, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "# Grant permissions for all users\n",
        "print(\"\\nüîê Assigning CAN MANAGE Permissions:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "permission_results = []\n",
        "for user in user_data:\n",
        "    result = grant_catalog_permissions(user['catalog_name'], user['email'], dry_run=dry_run)\n",
        "    permission_results.append(result)\n",
        "    user['permissions_granted'] = result['granted']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "success_count = sum(1 for r in permission_results if r.get('granted', False))\n",
        "print(f\"\\nüìä Summary: {success_count}/{len(permission_results)} permission grants successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Volumes with Consistent Naming\n",
        "\n",
        "Create a volume in each catalog with a consistent name across all catalogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_schema_and_volume(catalog_name, schema_name, volume_name, dry_run=False):\n",
        "    \"\"\"\n",
        "    Create schema and volume in a catalog.\n",
        "    \n",
        "    Args:\n",
        "        catalog_name: Name of the catalog\n",
        "        schema_name: Name of the schema\n",
        "        volume_name: Name of the volume\n",
        "        dry_run: If True, only preview without executing\n",
        "    \n",
        "    Returns:\n",
        "        Creation status dict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        volume_path = f\"{catalog_name}.{schema_name}.{volume_name}\"\n",
        "        \n",
        "        if dry_run:\n",
        "            print(f\"   ‚ö†Ô∏è  DRY RUN: Would create schema '{catalog_name}.{schema_name}' and volume '{volume_path}'\")\n",
        "            return {\n",
        "                \"catalog\": catalog_name,\n",
        "                \"volume_path\": volume_path,\n",
        "                \"created\": False,\n",
        "                \"status\": \"preview\"\n",
        "            }\n",
        "        \n",
        "        # Create schema if not exists\n",
        "        spark.sql(f\"\"\"\n",
        "            CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.`{schema_name}`\n",
        "            COMMENT 'Schema for user data and volumes'\n",
        "        \"\"\")\n",
        "        \n",
        "        # Create volume\n",
        "        spark.sql(f\"\"\"\n",
        "            CREATE VOLUME IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`{volume_name}`\n",
        "            COMMENT 'User data volume'\n",
        "        \"\"\")\n",
        "        \n",
        "        print(f\"   ‚úÖ Created volume: {volume_path}\")\n",
        "        return {\n",
        "            \"catalog\": catalog_name,\n",
        "            \"volume_path\": volume_path,\n",
        "            \"created\": True,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error creating volume in {catalog_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"catalog\": catalog_name,\n",
        "            \"volume_path\": volume_path,\n",
        "            \"created\": False,\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Create volumes for all users\n",
        "print(\"\\nüíæ Creating User Volumes:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "volume_results = []\n",
        "for user in user_data:\n",
        "    result = create_schema_and_volume(\n",
        "        user['catalog_name'],\n",
        "        schema_name,\n",
        "        volume_name,\n",
        "        dry_run\n",
        "    )\n",
        "    volume_results.append(result)\n",
        "    user['volume_name'] = result['volume_path']\n",
        "    user['volume_created'] = result['created']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "success_count = sum(1 for r in volume_results if r.get('created', False))\n",
        "print(f\"\\nüìä Summary: {success_count}/{len(volume_results)} volumes created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Load Data to Volumes\n",
        "\n",
        "Copy sample data from the source path to each user's volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_to_volume(volume_path, source_path, dry_run=False):\n",
        "    \"\"\"\n",
        "    Copy data from source to volume.\n",
        "    \n",
        "    Args:\n",
        "        volume_path: Full volume path (catalog.schema.volume)\n",
        "        source_path: Source data path to copy from\n",
        "        dry_run: If True, only preview without executing\n",
        "    \n",
        "    Returns:\n",
        "        Data loading status dict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert volume path to file system path\n",
        "        volume_fs_path = f\"/Volumes/{volume_path.replace('.', '/')}\"\n",
        "        \n",
        "        if dry_run:\n",
        "            print(f\"   ‚ö†Ô∏è  DRY RUN: Would copy data from '{source_path}' to '{volume_fs_path}'\")\n",
        "            return {\n",
        "                \"volume\": volume_path,\n",
        "                \"data_location\": volume_fs_path,\n",
        "                \"loaded\": False,\n",
        "                \"status\": \"preview\"\n",
        "            }\n",
        "        \n",
        "        # Copy files using dbutils\n",
        "        # Note: In production, you would implement actual file copying logic\n",
        "        # For example: dbutils.fs.cp(source_path, volume_fs_path, recurse=True)\n",
        "        \n",
        "        # Simulate data loading\n",
        "        data_location = f\"{volume_fs_path}/data\"\n",
        "        \n",
        "        # Create a simple marker file to indicate data was loaded\n",
        "        dbutils.fs.mkdirs(data_location)\n",
        "        \n",
        "        print(f\"   ‚úÖ Data loaded to: {data_location}\")\n",
        "        return {\n",
        "            \"volume\": volume_path,\n",
        "            \"data_location\": data_location,\n",
        "            \"loaded\": True,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error loading data to {volume_path}: {str(e)}\")\n",
        "        return {\n",
        "            \"volume\": volume_path,\n",
        "            \"data_location\": None,\n",
        "            \"loaded\": False,\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Load data to all volumes\n",
        "print(\"\\nüì• Loading Data to Volumes:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "data_load_results = []\n",
        "for user in user_data:\n",
        "    if user.get('volume_created', False) or dry_run:\n",
        "        result = load_data_to_volume(\n",
        "            user['volume_name'],\n",
        "            source_data_path,\n",
        "            dry_run\n",
        "        )\n",
        "        data_load_results.append(result)\n",
        "        user['data_location'] = result['data_location']\n",
        "        user['data_loaded'] = result['loaded']\n",
        "    else:\n",
        "        print(f\"   ‚è≠Ô∏è  Skipping data load for {user['volume_name']} (volume not created)\")\n",
        "        user['data_location'] = None\n",
        "        user['data_loaded'] = False\n",
        "\n",
        "print(\"=\" * 80)\n",
        "success_count = sum(1 for r in data_load_results if r.get('loaded', False))\n",
        "print(f\"\\nüìä Summary: {success_count}/{len(data_load_results)} data loads successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Provisioning Report Table\n",
        "\n",
        "Create a comprehensive report table showing all provisioning details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
        "\n",
        "# Define schema for the report\n",
        "report_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), False),\n",
        "    StructField(\"full_name\", StringType(), False),\n",
        "    StructField(\"user_alias\", StringType(), False),\n",
        "    StructField(\"catalog_created\", StringType(), False),\n",
        "    StructField(\"permissions_assigned\", BooleanType(), False),\n",
        "    StructField(\"volume_name\", StringType(), True),\n",
        "    StructField(\"volume_data_location\", StringType(), True),\n",
        "    StructField(\"provisioning_status\", StringType(), False)\n",
        "])\n",
        "\n",
        "# Build report rows\n",
        "report_rows = []\n",
        "for idx, user in enumerate(user_data, 1):\n",
        "    # Determine overall provisioning status\n",
        "    if user.get('catalog_created', False) and user.get('permissions_granted', False) and user.get('volume_created', False) and user.get('data_loaded', False):\n",
        "        status = \"‚úÖ Complete\"\n",
        "    elif dry_run:\n",
        "        status = \"‚ö†Ô∏è Preview Only\"\n",
        "    else:\n",
        "        status = \"‚ö†Ô∏è Partial\"\n",
        "    \n",
        "    report_rows.append(\n",
        "        Row(\n",
        "            user_id=user['email'],\n",
        "            full_name=user['full_name'],\n",
        "            user_alias=user['alias'],\n",
        "            catalog_created=user['catalog_name'],\n",
        "            permissions_assigned=user.get('permissions_granted', False),\n",
        "            volume_name=user.get('volume_name', None),\n",
        "            volume_data_location=user.get('data_location', None),\n",
        "            provisioning_status=status\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create DataFrame\n",
        "report_df = spark.createDataFrame(report_rows, schema=report_schema)\n",
        "\n",
        "# Display report\n",
        "print(\"\\nüìä Provisioning Report:\")\n",
        "print(\"=\" * 80)\n",
        "display(report_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "total_users = len(user_data)\n",
        "catalogs_created = sum(1 for u in user_data if u.get('catalog_created', False))\n",
        "permissions_granted = sum(1 for u in user_data if u.get('permissions_granted', False))\n",
        "volumes_created = sum(1 for u in user_data if u.get('volume_created', False))\n",
        "data_loaded = sum(1 for u in user_data if u.get('data_loaded', False))\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà WORKSHOP PROVISIONING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total Users Processed:        {total_users}\")\n",
        "print(f\"Catalogs Created:             {catalogs_created}/{total_users} ({catalogs_created/total_users*100:.1f}%)\")\n",
        "print(f\"Permissions Granted:          {permissions_granted}/{total_users} ({permissions_granted/total_users*100:.1f}%)\")\n",
        "print(f\"Volumes Created:              {volumes_created}/{total_users} ({volumes_created/total_users*100:.1f}%)\")\n",
        "print(f\"Data Loaded:                  {data_loaded}/{total_users} ({data_loaded/total_users*100:.1f}%)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Success check\n",
        "if dry_run:\n",
        "    print(\"\\n‚ö†Ô∏è  DRY RUN MODE: No changes were made. Set 'Dry Run' to 'No' to execute.\")\n",
        "elif catalogs_created == total_users and permissions_granted == total_users and volumes_created == total_users and data_loaded == total_users:\n",
        "    print(\"\\n‚úÖ SUCCESS: All users provisioned successfully!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Some provisioning steps failed. Review the report above for details.\")\n",
        "    \n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Export Report (Optional)\n",
        "\n",
        "Save the provisioning report to a Delta table for audit purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save report to Delta table (configuration from config.yaml)\n",
        "save_report = config.get(\"save_report_to_delta\", \"Yes\") == \"Yes\"\n",
        "report_catalog = config.get(\"report_catalog\", \"main\")\n",
        "report_schema_name = config.get(\"report_schema\", \"default\")\n",
        "report_table = config.get(\"report_table\", \"provisioning_reports\")\n",
        "\n",
        "if not dry_run and save_report:\n",
        "    try:\n",
        "        # Add timestamp\n",
        "        from pyspark.sql.functions import current_timestamp, lit\n",
        "        \n",
        "        report_df_with_timestamp = report_df \\\n",
        "            .withColumn(\"provisioning_timestamp\", current_timestamp()) \\\n",
        "            .withColumn(\"base_catalog_name\", lit(base_catalog_name))\n",
        "        \n",
        "        report_table_path = f\"{report_catalog}.{report_schema_name}.{report_table}\"\n",
        "        \n",
        "        # Write to Delta table (append mode for historical tracking)\n",
        "        report_df_with_timestamp.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .mode(\"append\") \\\n",
        "            .saveAsTable(report_table_path)\n",
        "        \n",
        "        print(f\"\\nüíæ Report saved to table: {report_table_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Could not save report to Delta table: {str(e)}\")\n",
        "        print(\"   Report is still available in the notebook output above.\")\n",
        "else:\n",
        "    if dry_run:\n",
        "        print(\"\\n‚è≠Ô∏è  Skipping report save (dry run mode)\")\n",
        "    else:\n",
        "        print(\"\\n‚è≠Ô∏è  Skipping report save (disabled in config.yaml)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Workshop Setup Complete\n",
        "\n",
        "This notebook has completed the user provisioning workflow following the HEDIS measure agent configuration pattern.\n",
        "\n",
        "### Next Steps\n",
        "1. Review the provisioning report table above\n",
        "2. Verify catalog and volume creation in Unity Catalog UI\n",
        "3. Test user access to their assigned catalogs\n",
        "4. Load production data to volumes as needed\n",
        "\n",
        "### Configuration Pattern Benefits\n",
        "- ‚úÖ Config values loaded from config.yaml (HEDIS pattern)\n",
        "- ‚úÖ Widget-based parameter override capability\n",
        "- ‚úÖ Type-safe parameter handling\n",
        "- ‚úÖ Clear execution flow with status reporting\n",
        "- ‚úÖ Dry run capability for safe testing\n",
        "- ‚úÖ Comprehensive audit trail in report table"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Workshop Setup",
      "dashboards": [],
      "language": "python",
      "widgets": {},
      "notebookMetadata": {
        "pythonIndentUnit": 2
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
