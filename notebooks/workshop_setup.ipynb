{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aebc11c-297b-4fdf-9d03-1f89496ce380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop Setup - User Provisioning and Catalog Management\n",
    "\n",
    "This notebook automates the provisioning of Unity Catalog resources for multiple users.\n",
    "\n",
    "## Workflow\n",
    "1. Load configuration from config.yaml\n",
    "2. Parse user list and generate user aliases\n",
    "3. Configure Delta Share recipient\n",
    "4. Create user-specific catalogs with naming convention\n",
    "5. Assign CAN MANAGE permissions to catalog owners\n",
    "6. Create volumes with consistent naming\n",
    "7. Load data to volumes\n",
    "8. Generate provisioning report table\n",
    "\n",
    "## Configuration Pattern\n",
    "- Config values loaded from config.yaml\n",
    "- Widget-based parameter override capability\n",
    "- Type-safe parameter handling\n",
    "- Clear execution flow with status reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a49c21a-2621-46f0-bf29-e2ffbaee3822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Install required packages from requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de9d22d-c858-4ca1-9a70-4e2d98adca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffdd0ddf-3c8f-42f8-a62d-3d568a952fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Configuration from config.yaml\n",
    "\n",
    "Load configuration file and use values as widget defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d4b5d9-6580-4fe4-818e-ecbe0f80cd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "# Support multiple execution contexts (local, workspace, etc.)\n",
    "config_paths = [\n",
    "    \"../config.yaml\",  # Relative path from notebooks/ directory\n",
    "    # \"/Workspace/Repos/dbx-sdp-workshop/config.yaml\",  # Workspace path\n",
    "    # \"config.yaml\"  # Current directory fallback\n",
    "]\n",
    "\n",
    "config = None\n",
    "config_loaded_from = None\n",
    "\n",
    "for config_path in config_paths:\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            config_loaded_from = config_path\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading config from {config_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if config is None:\n",
    "    print(\"‚ö†Ô∏è  Warning: Could not load config.yaml, using default values\")\n",
    "    config = {}\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_loaded_from}\")\n",
    "    print(f\"   Configuration keys: {list(config.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba40ddd-87e0-4f02-94b0-78bde3181e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Configuration Widgets\n",
    "\n",
    "Create widgets with defaults from config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61cbcf2-3f24-41fb-9876-eec2ac8c4289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget Configuration\n",
    "# Config.yaml provides default values for all widgets\n",
    "\n",
    "# User Configuration\n",
    "dbutils.widgets.text(\"user_list\", config.get(\"user_list\", \"marcin.jimenez@databricks.com\"), \"User List (comma-separated emails)\")\n",
    "\n",
    "# Delta Share Configuration\n",
    "dbutils.widgets.text(\"delta_share_file\", config.get(\"delta_share_file\", \"config.share\"), \"Delta Share Configuration File Path\")\n",
    "dbutils.widgets.text(\"delta_share_provider\", config.get(\"delta_share_provider\", \"azure:eastus2:databricks:field-eng-east\"), \"Delta Share Provider Identifier\")\n",
    "dbutils.widgets.text(\"delta_share_name\", config.get(\"delta_share_name\", \"scp-demo\"), \"Delta Share Name\")\n",
    "dbutils.widgets.text(\"delta_share_catalog\", config.get(\"delta_share_catalog\", \"shared_catalog\"), \"Delta Share Catalog Name (local)\")\n",
    "dbutils.widgets.text(\"delta_share_schema\", config.get(\"delta_share_schema\", \"shared_schema\"), \"Delta Share Schema Name\")\n",
    "dbutils.widgets.text(\"delta_share_volume\", config.get(\"delta_share_volume\", \"shared_volume\"), \"Delta Share Volume Name\")\n",
    "\n",
    "# File Pattern Configuration\n",
    "dbutils.widgets.text(\"file_glob_pattern\", config.get(\"file_glob_pattern\", \"*.parquet\"), \"File Glob Pattern to Copy\")\n",
    "\n",
    "# Catalog Configuration\n",
    "dbutils.widgets.text(\"base_catalog_name\", config.get(\"base_catalog_name\", \"workshop_catalog\"), \"Base Catalog Name\")\n",
    "\n",
    "# Volume Configuration\n",
    "dbutils.widgets.text(\"volume_name\", config.get(\"volume_name\", \"user_data_volume\"), \"Volume Name (consistent across all catalogs)\")\n",
    "dbutils.widgets.text(\"schema_name\", config.get(\"schema_name\", \"default\"), \"Schema Name for Volumes\")\n",
    "\n",
    "# Data Source Configuration\n",
    "dbutils.widgets.text(\"source_data_path\", config.get(\"source_data_path\", \"/databricks-datasets/sample_data\"), \"Source Data Path to Copy to Volumes\")\n",
    "\n",
    "print(\"‚úÖ Configuration widgets created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530f30b5-e9a3-4463-b1cc-f1c82a1fe52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load and Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9500d3-fb07-4dd8-b103-3eee686ff7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve widget values (widgets override config.yaml if changed by user)\n",
    "user_list_raw = dbutils.widgets.get(\"user_list\")\n",
    "delta_share_file = dbutils.widgets.get(\"delta_share_file\")\n",
    "delta_share_provider = dbutils.widgets.get(\"delta_share_provider\")\n",
    "delta_share_name = dbutils.widgets.get(\"delta_share_name\")\n",
    "delta_share_catalog = dbutils.widgets.get(\"delta_share_catalog\")\n",
    "delta_share_schema = dbutils.widgets.get(\"delta_share_schema\")\n",
    "delta_share_volume = dbutils.widgets.get(\"delta_share_volume\")\n",
    "file_glob_pattern = dbutils.widgets.get(\"file_glob_pattern\")\n",
    "base_catalog_name = dbutils.widgets.get(\"base_catalog_name\")\n",
    "volume_name = dbutils.widgets.get(\"volume_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "source_data_path = dbutils.widgets.get(\"source_data_path\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"üîß Workshop Provisioning Configuration:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Config Source:       {config_loaded_from or 'defaults'}\")\n",
    "print(f\"User List:           {user_list_raw}\")\n",
    "print(f\"Delta Share:         {delta_share_provider}.{delta_share_name}\")\n",
    "print(f\"Delta Share Catalog: {delta_share_catalog} (mounting to local workspace)\")\n",
    "print(f\"Delta Share Source:  {delta_share_catalog}.{delta_share_schema}.{delta_share_volume}\")\n",
    "print(f\"File Glob Pattern:   {file_glob_pattern}\")\n",
    "print(f\"Base Catalog:        {base_catalog_name}\")\n",
    "print(f\"Volume Name:         {volume_name}\")\n",
    "print(f\"Schema Name:         {schema_name}\")\n",
    "print(f\"Fallback Source:     {source_data_path}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723485eb-8128-4e1f-89f3-9561058f5206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Parse Users and Generate Aliases\n",
    "\n",
    "Parse the comma-separated user list and generate user aliases using the pattern:\n",
    "- First 3 letters of first name\n",
    "- First 4 letters of last name\n",
    "- Concatenated with underscore\n",
    "- Example: \"John Smith\" ‚Üí \"joh_smit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7401b9-00d7-4a4d-9324-cca09ce314a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_user_alias(email):\n",
    "    \"\"\"\n",
    "    Generate user alias from email address.\n",
    "    Pattern: first 3 letters of first name + _ + first 4 letters of last name\n",
    "    \n",
    "    Args:\n",
    "        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n",
    "    \n",
    "    Returns:\n",
    "        User alias (e.g., \"mar_jime\")\n",
    "    \"\"\"\n",
    "    # Extract username part from email (before @)\n",
    "    if '@' not in email:\n",
    "        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com'\")\n",
    "    \n",
    "    username = email.split('@')[0]\n",
    "    \n",
    "    # Split by dot or underscore to get first and last name\n",
    "    name_parts = re.split(r'[._]', username)\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com' or 'first_last@domain.com'\")\n",
    "    \n",
    "    first_name = name_parts[0].lower()\n",
    "    last_name = name_parts[-1].lower()  # Use last part in case of middle names\n",
    "    \n",
    "    # Generate alias: first 3 chars of first name + first 4 chars of last name\n",
    "    first_part = first_name[:3]\n",
    "    last_part = last_name[:4]\n",
    "    \n",
    "    alias = f\"{first_part}{last_part}\"\n",
    "    \n",
    "    # Remove any non-alphanumeric characters except underscore\n",
    "    alias = re.sub(r'[^a-z0-9_]', '', alias)\n",
    "    \n",
    "    return alias\n",
    "\n",
    "def extract_full_name(email):\n",
    "    \"\"\"\n",
    "    Extract full name from email address.\n",
    "    \n",
    "    Args:\n",
    "        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n",
    "    \n",
    "    Returns:\n",
    "        Full name with proper capitalization (e.g., \"Marcin Jimenez\")\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    name_parts = re.split(r'[._]', username)\n",
    "    \n",
    "    # Capitalize each part\n",
    "    full_name = ' '.join(part.capitalize() for part in name_parts)\n",
    "    \n",
    "    return full_name\n",
    "\n",
    "# Parse user list (expecting comma-separated email addresses)\n",
    "users = [user.strip() for user in user_list_raw.split(',') if user.strip()]\n",
    "\n",
    "# Validate and generate user data structure\n",
    "user_data = []\n",
    "errors = []\n",
    "\n",
    "for email in users:\n",
    "    try:\n",
    "        alias = generate_user_alias(email)\n",
    "        full_name = extract_full_name(email)\n",
    "        \n",
    "        user_data.append({\n",
    "            \"full_name\": full_name,\n",
    "            \"alias\": alias,\n",
    "            \"catalog_name\": f\"{base_catalog_name}_{alias}\",\n",
    "            \"email\": email\n",
    "        })\n",
    "    except ValueError as e:\n",
    "        errors.append(str(e))\n",
    "\n",
    "# If there are any errors, raise an exception\n",
    "if errors:\n",
    "    error_message = \"‚ùå User parsing failed:\\n\" + \"\\n\".join(f\"   - {err}\" for err in errors)\n",
    "    raise ValueError(error_message)\n",
    "\n",
    "# Display parsed users\n",
    "print(f\"\\nüìã Parsed {len(user_data)} users:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, user in enumerate(user_data, 1):\n",
    "    print(f\"{idx}. {user['email']:40s} ‚Üí {user['full_name']:20s} ‚Üí Alias: {user['alias']:12s} ‚Üí Catalog: {user['catalog_name']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2de51a-93bf-4cf0-bd6b-60647699a10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_delta_share_catalog(catalog_name, provider, share_name):\n",
    "    \"\"\"\n",
    "    Mount a Delta Share to a local catalog.\n",
    "    \n",
    "    Args:\n",
    "        catalog_name: Local catalog name to create\n",
    "        provider: Delta Share provider identifier (e.g., \"azure:eastus2:databricks:field-eng-east\")\n",
    "        share_name: Name of the share (e.g., \"scp-demo\")\n",
    "    \n",
    "    Returns:\n",
    "        Mount status dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if catalog already exists\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE CATALOG `{catalog_name}`\")\n",
    "            print(f\"‚úÖ Delta Share catalog '{catalog_name}' already mounted\")\n",
    "            return {\"status\": \"already_exists\", \"mounted\": True, \"catalog\": catalog_name}\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Create catalog from Delta Share\n",
    "        share_path = f\"{provider}.{share_name}\"\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE CATALOG IF NOT EXISTS `{catalog_name}`\n",
    "            USING SHARE `{share_path}`\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"‚úÖ Mounted Delta Share '{share_path}' to catalog '{catalog_name}'\")\n",
    "        return {\"status\": \"success\", \"mounted\": True, \"catalog\": catalog_name}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mounting Delta Share: {str(e)}\")\n",
    "        print(f\"   Verify share exists: {provider}.{share_name}\")\n",
    "        return {\"status\": \"error\", \"mounted\": False, \"error\": str(e)}\n",
    "\n",
    "# Mount Delta Share\n",
    "print(\"\\nüì¶ Mounting Delta Share to Catalog:\")\n",
    "print(\"=\" * 80)\n",
    "mount_status = mount_delta_share_catalog(delta_share_catalog, delta_share_provider, delta_share_name)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d57691b7-3d5c-4b6a-8f29-8cfbbbf97587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Mount Delta Share to Catalog\n",
    "\n",
    "Mount the Delta Share to a local catalog for accessing shared data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c7f5a1-a0ad-4461-8ac0-13686774d1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Create User Catalogs\n",
    "\n",
    "Create Unity Catalog catalogs for each user with the naming pattern: `{base_catalog_name}_{user_alias}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2983e5-bf8c-4059-b7bf-5674d2376204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_catalog(catalog_name, comment):\n",
    "    \"\"\"Create a Unity Catalog catalog.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog_name}` COMMENT '{comment}'\")\n",
    "        return {\"catalog\": catalog_name, \"created\": True, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating catalog {catalog_name}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Create catalogs for all users\n",
    "print(\"\\nüìö Creating User Catalogs:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "catalog_results = []\n",
    "for user in user_data:\n",
    "    comment = f\"Catalog for user {user['full_name']} ({user['alias']})\"\n",
    "    result = create_catalog(user['catalog_name'], comment)\n",
    "    catalog_results.append(result)\n",
    "    user['catalog_created'] = result['created']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in catalog_results if r.get('created', False))\n",
    "print(f\"üìä Created {success_count}/{len(catalog_results)} catalogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a38e08-c8cd-4bfe-925f-f84163320626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def grant_catalog_permissions(catalog_name, user_email):\n",
    "    \"\"\"Grant CAN MANAGE permissions on a catalog to a user.\"\"\"\n",
    "    try:\n",
    "        for priv in [\"USE CATALOG\", \"USE SCHEMA\", \"CREATE SCHEMA\"]:\n",
    "            spark.sql(f\"GRANT {priv} ON CATALOG `{catalog_name}` TO `{user_email}`\")\n",
    "        \n",
    "        spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG `{catalog_name}` TO `{user_email}`\")\n",
    "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": True, \"status\": \"success\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error granting permissions on {catalog_name} to {user_email}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Grant permissions for all users\n",
    "print(\"\\nüîê Assigning CAN MANAGE Permissions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "permission_results = []\n",
    "for user in user_data:\n",
    "    result = grant_catalog_permissions(user['catalog_name'], user['email'])\n",
    "    permission_results.append(result)\n",
    "    user['permissions_granted'] = result['granted']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in permission_results if r.get('granted', False))\n",
    "print(f\"üìä Granted permissions to {success_count}/{len(permission_results)} catalogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2e3c02-5eb5-4baa-8971-7e1f61f3de30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Assign CAN MANAGE Permissions\n",
    "\n",
    "Grant CAN MANAGE permissions to each user for their respective catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9f32c8-502a-4477-97ae-b2a91a6cb240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Create Volumes with Consistent Naming\n",
    "\n",
    "Create a volume in each catalog with a consistent name across all catalogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a5bf39-e00c-4405-bb79-27a4d8419796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Load Data to Volumes from Delta Share\n",
    "\n",
    "Copy data from Delta Share volume to each user's volume using the glob pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bf53c3f-bb11-4584-93ad-043e2c695722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "def load_data_to_volume(volume_path, delta_share_catalog, delta_share_schema, delta_share_volume, file_pattern, fallback_source):\n",
    "    \"\"\"Copy data from Delta Share volume to user volume using glob pattern.\"\"\"\n",
    "    try:\n",
    "        volume_fs_path = f\"/Volumes/{volume_path.replace('.', '/')}\"\n",
    "        data_location = f\"{volume_fs_path}/data\"\n",
    "        dbutils.fs.mkdirs(data_location)\n",
    "        \n",
    "        # Try Delta Share volume first\n",
    "        delta_share_volume_path = f\"/Volumes/{delta_share_catalog}/{delta_share_schema}/{delta_share_volume}\"\n",
    "        \n",
    "        try:\n",
    "            dbutils.fs.ls(delta_share_volume_path)\n",
    "            source_path = delta_share_volume_path\n",
    "            using_delta_share = True\n",
    "        except Exception:\n",
    "            source_path = fallback_source\n",
    "            using_delta_share = False\n",
    "        \n",
    "        # List and filter files\n",
    "        try:\n",
    "            all_files = dbutils.fs.ls(source_path)\n",
    "            matching_files = [f.path for f in all_files if fnmatch.fnmatch(f.name, file_pattern)]\n",
    "            \n",
    "            if not matching_files:\n",
    "                return {\"volume\": volume_path, \"data_location\": data_location, \"loaded\": False, \"status\": \"no_files_found\", \"files_copied\": 0}\n",
    "            \n",
    "            # Copy files\n",
    "            files_copied = 0\n",
    "            for file_path in matching_files:\n",
    "                file_name = file_path.split('/')[-1]\n",
    "                dest_path = f\"{data_location}/{file_name}\"\n",
    "                try:\n",
    "                    dbutils.fs.cp(file_path, dest_path)\n",
    "                    files_copied += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            return {\n",
    "                \"volume\": volume_path,\n",
    "                \"data_location\": data_location,\n",
    "                \"loaded\": True,\n",
    "                \"status\": \"success\",\n",
    "                \"files_copied\": files_copied,\n",
    "                \"source\": \"delta_share\" if using_delta_share else \"fallback\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"volume\": volume_path, \"data_location\": data_location, \"loaded\": False, \"status\": \"error_listing\", \"error\": str(e)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data to {volume_path}: {str(e)}\")\n",
    "        return {\"volume\": volume_path, \"data_location\": None, \"loaded\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Load data to all volumes\n",
    "print(\"\\nüì• Loading Data to Volumes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_load_results = []\n",
    "for user in user_data:\n",
    "    if user.get('volume_created', False):\n",
    "        result = load_data_to_volume(\n",
    "            user['volume_name'],\n",
    "            delta_share_catalog,\n",
    "            delta_share_schema,\n",
    "            delta_share_volume,\n",
    "            file_glob_pattern,\n",
    "            source_data_path\n",
    "        )\n",
    "        data_load_results.append(result)\n",
    "        user['data_location'] = result['data_location']\n",
    "        user['data_loaded'] = result['loaded']\n",
    "        user['files_copied'] = result.get('files_copied', 0)\n",
    "    else:\n",
    "        user['data_location'] = None\n",
    "        user['data_loaded'] = False\n",
    "        user['files_copied'] = 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in data_load_results if r.get('loaded', False))\n",
    "total_files = sum(r.get('files_copied', 0) for r in data_load_results)\n",
    "delta_share_used = sum(1 for r in data_load_results if r.get('source') == 'delta_share')\n",
    "\n",
    "print(f\"üìä Loaded data to {success_count}/{len(data_load_results)} volumes\")\n",
    "print(f\"   Total files copied: {total_files}\")\n",
    "if delta_share_used > 0:\n",
    "    print(f\"   Delta Share used for {delta_share_used} volumes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f74b13-d6e6-4683-887f-30316693f51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema_and_volume(catalog_name, schema_name, volume_name):\n",
    "    \"\"\"Create schema and volume in a catalog.\"\"\"\n",
    "    try:\n",
    "        volume_path = f\"{catalog_name}.{schema_name}.{volume_name}\"\n",
    "        \n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.`{schema_name}` COMMENT 'Schema for user data and volumes'\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`{volume_name}` COMMENT 'User data volume'\")\n",
    "        \n",
    "        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": True, \"status\": \"success\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating volume in {catalog_name}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Create volumes for all users\n",
    "print(\"\\nüíæ Creating User Volumes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "volume_results = []\n",
    "for user in user_data:\n",
    "    result = create_schema_and_volume(user['catalog_name'], schema_name, volume_name)\n",
    "    volume_results.append(result)\n",
    "    user['volume_name'] = result['volume_path']\n",
    "    user['volume_created'] = result['created']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in volume_results if r.get('created', False))\n",
    "print(f\"üìä Created {success_count}/{len(volume_results)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4152983a-d5ab-4e39-9042-e3886b8ca145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Generate Provisioning Report Table\n",
    "\n",
    "Create a comprehensive report table showing all provisioning details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b259d2f7-8fde-45aa-b7ee-b5c02b8900bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "# Define schema for the report\n",
    "report_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), False),\n",
    "    StructField(\"user_alias\", StringType(), False),\n",
    "    StructField(\"catalog_created\", StringType(), False),\n",
    "    StructField(\"permissions_assigned\", BooleanType(), False),\n",
    "    StructField(\"volume_name\", StringType(), True),\n",
    "    StructField(\"volume_data_location\", StringType(), True),\n",
    "    StructField(\"provisioning_status\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Build report rows\n",
    "report_rows = []\n",
    "for idx, user in enumerate(user_data, 1):\n",
    "    # Determine overall provisioning status\n",
    "    if user.get('catalog_created', False) and user.get('permissions_granted', False) and user.get('volume_created', False) and user.get('data_loaded', False):\n",
    "        status = \"‚úÖ Complete\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è Partial\"\n",
    "    \n",
    "    report_rows.append(\n",
    "        Row(\n",
    "            user_id=user['email'],\n",
    "            full_name=user['full_name'],\n",
    "            user_alias=user['alias'],\n",
    "            catalog_created=user['catalog_name'],\n",
    "            permissions_assigned=user.get('permissions_granted', False),\n",
    "            volume_name=user.get('volume_name', None),\n",
    "            volume_data_location=user.get('data_location', None),\n",
    "            provisioning_status=status\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create DataFrame\n",
    "report_df = spark.createDataFrame(report_rows, schema=report_schema)\n",
    "\n",
    "# Display report\n",
    "print(\"\\nüìä Provisioning Report:\")\n",
    "print(\"=\" * 80)\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d52103-2c27-4b8d-91a3-3288cb1d5ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5894966d-453e-49a0-9c94-d6566463dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_users = len(user_data)\n",
    "catalogs_created = sum(1 for u in user_data if u.get('catalog_created', False))\n",
    "permissions_granted = sum(1 for u in user_data if u.get('permissions_granted', False))\n",
    "volumes_created = sum(1 for u in user_data if u.get('volume_created', False))\n",
    "data_loaded = sum(1 for u in user_data if u.get('data_loaded', False))\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà WORKSHOP PROVISIONING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Users Processed:        {total_users}\")\n",
    "print(f\"Catalogs Created:             {catalogs_created}/{total_users} ({catalogs_created/total_users*100:.1f}%)\")\n",
    "print(f\"Permissions Granted:          {permissions_granted}/{total_users} ({permissions_granted/total_users*100:.1f}%)\")\n",
    "print(f\"Volumes Created:              {volumes_created}/{total_users} ({volumes_created/total_users*100:.1f}%)\")\n",
    "print(f\"Data Loaded:                  {data_loaded}/{total_users} ({data_loaded/total_users*100:.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Success check\n",
    "if catalogs_created == total_users and permissions_granted == total_users and volumes_created == total_users and data_loaded == total_users:\n",
    "    print(\"\\n‚úÖ SUCCESS: All users provisioned successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Some provisioning steps failed. Review the report above for details.\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84285cd6-60c2-49b7-ae05-02fc90529b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Export Report (Optional)\n",
    "\n",
    "Save the provisioning report to a Delta table for audit purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "237e04be-26ce-4052-9e5f-f75c0c335f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save report to Delta table and output file\n",
    "save_report = config.get(\"save_report_to_delta\", \"Yes\") == \"Yes\"\n",
    "report_catalog = config.get(\"report_catalog\", \"main\")\n",
    "report_schema_name = config.get(\"report_schema\", \"default\")\n",
    "report_table = config.get(\"report_table\", \"provisioning_reports\")\n",
    "\n",
    "# Always save report to file for cleanup tracking\n",
    "report_output_file = \"../provisioning_report.json\"\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    \n",
    "    report_data = {\n",
    "        \"base_catalog_name\": base_catalog_name,\n",
    "        \"users\": user_data,\n",
    "        \"timestamp\": str(report_df.select(\"provisioning_timestamp\").first()[0]) if \"provisioning_timestamp\" in report_df.columns else None\n",
    "    }\n",
    "    \n",
    "    with open(report_output_file, 'w') as f:\n",
    "        json.dump(report_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Report saved to: {report_output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not save report to file: {str(e)}\")\n",
    "\n",
    "# Optionally save to Delta table\n",
    "if save_report:\n",
    "    try:\n",
    "        from pyspark.sql.functions import current_timestamp, lit\n",
    "        \n",
    "        report_df_with_timestamp = report_df \\\n",
    "            .withColumn(\"provisioning_timestamp\", current_timestamp()) \\\n",
    "            .withColumn(\"base_catalog_name\", lit(base_catalog_name))\n",
    "        \n",
    "        report_table_path = f\"{report_catalog}.{report_schema_name}.{report_table}\"\n",
    "        report_df_with_timestamp.write.format(\"delta\").mode(\"append\").saveAsTable(report_table_path)\n",
    "        \n",
    "        print(f\"üíæ Report saved to: {report_table_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not save to Delta table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706b567b-749f-4c21-9edc-527732ac3660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup Section - Delete Provisioned Resources\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: The following cells will DELETE all catalogs created by this workshop setup.\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the first cell to load the provisioning report\n",
    "2. Run the second cell and type \"CONFIRM\" in the widget to delete all resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bedfc8dc-264b-43ed-9ff1-8dae18b3223a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Try to load the provisioning report\n",
    "report_file_paths = [\n",
    "    \"../provisioning_report.json\",\n",
    "    \"/Workspace/Repos/dbx-sdp-workshop/provisioning_report.json\",\n",
    "    \"provisioning_report.json\"\n",
    "]\n",
    "\n",
    "cleanup_data = None\n",
    "report_file_loaded = None\n",
    "\n",
    "for path in report_file_paths:\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            cleanup_data = json.load(f)\n",
    "            report_file_loaded = path\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading {path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if cleanup_data is None:\n",
    "    print(\"‚ùå No provisioning report found!\")\n",
    "    print(\"   Run the provisioning cells above first to create resources.\")\n",
    "    print(\"   The report file (provisioning_report.json) is required for cleanup.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Provisioning report loaded from: {report_file_loaded}\")\n",
    "    print(f\"   Base catalog name: {cleanup_data.get('base_catalog_name')}\")\n",
    "    print(f\"   Users found: {len(cleanup_data.get('users', []))}\")\n",
    "    print(f\"   Timestamp: {cleanup_data.get('timestamp', 'N/A')}\")\n",
    "    \n",
    "    # Display catalogs to be deleted\n",
    "    print(f\"\\nüìã Catalogs that will be deleted:\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, user in enumerate(cleanup_data.get('users', []), 1):\n",
    "        print(f\"{idx}. {user['catalog_name']}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e749caf-4bb7-407f-9ca0-28c207ce7f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create confirmation widget if it doesn't exist\n",
    "try:\n",
    "    dbutils.widgets.get(\"delete_confirmation\")\n",
    "except:\n",
    "    dbutils.widgets.text(\"delete_confirmation\", \"\", \"Type CONFIRM to delete all catalogs\")\n",
    "\n",
    "confirmation = dbutils.widgets.get(\"delete_confirmation\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  WARNING: This will permanently delete all provisioned catalogs and their contents!\")\n",
    "\n",
    "if confirmation != \"CONFIRM\":\n",
    "    print(\"\\n‚ùå Deletion cancelled - confirmation not provided\")\n",
    "    print(\"   Type 'CONFIRM' in the text field above and re-run this cell to proceed with deletion\")\n",
    "elif cleanup_data is None:\n",
    "    print(\"\\n‚ùå No provisioning report found - nothing to delete\")\n",
    "    print(\"   Run the cell above to load the provisioning report first.\")\n",
    "else:\n",
    "    print(\"\\nüóëÔ∏è  Starting catalog deletion process...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    deletion_results = []\n",
    "    \n",
    "    for user in cleanup_data.get('users', []):\n",
    "        catalog_name = user.get('catalog_name')\n",
    "        \n",
    "        try:\n",
    "            # Drop the catalog (CASCADE will delete all contents)\n",
    "            spark.sql(f\"DROP CATALOG IF EXISTS `{catalog_name}` CASCADE\")\n",
    "            print(f\"   ‚úÖ Deleted catalog: {catalog_name}\")\n",
    "            deletion_results.append({\n",
    "                \"catalog\": catalog_name,\n",
    "                \"deleted\": True,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error deleting catalog {catalog_name}: {str(e)}\")\n",
    "            deletion_results.append({\n",
    "                \"catalog\": catalog_name,\n",
    "                \"deleted\": False,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display summary\n",
    "    success_count = sum(1 for r in deletion_results if r.get('deleted', False))\n",
    "    total_count = len(deletion_results)\n",
    "    \n",
    "    print(f\"\\nüìä Deletion Summary:\")\n",
    "    print(f\"   Catalogs deleted: {success_count}/{total_count}\")\n",
    "    \n",
    "    if success_count == total_count:\n",
    "        print(\"\\n‚úÖ All catalogs deleted successfully!\")\n",
    "        \n",
    "        # Clean up the report file\n",
    "        try:\n",
    "            if report_file_loaded and os.path.exists(report_file_loaded):\n",
    "                os.remove(report_file_loaded)\n",
    "                print(f\"   üóëÔ∏è  Removed report file: {report_file_loaded}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not remove report file: {str(e)}\")\n",
    "        \n",
    "        # Remove confirmation widget\n",
    "        dbutils.widgets.remove(\"delete_confirmation\")\n",
    "        print(\"   üóëÔ∏è  Removed confirmation widget\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some catalogs could not be deleted. Review errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "206f5d58-1b3f-46a7-9a40-260b2d221d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup Section - Delete Provisioned Resources\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: The following cells will DELETE all catalogs created by this workshop setup.\n",
    "Only run these cells if you want to completely remove all provisioned resources."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workshop_setup",
   "widgets": {
    "base_catalog_name": {
     "currentValue": "workshop_catalog",
     "nuid": "cc58a4de-6c11-475f-b18a-4a9378a12b0c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workshop_catalog",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workshop_catalog",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_catalog": {
     "currentValue": "shared_catalog",
     "nuid": "a7f235cb-2e2c-4344-83f0-b2b45e99f658",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "shared_catalog",
      "label": "Delta Share Catalog Name (local)",
      "name": "delta_share_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "shared_catalog",
      "label": "Delta Share Catalog Name (local)",
      "name": "delta_share_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "delta_share_file": {
     "currentValue": "config.share",
     "nuid": "1c07f086-95bc-4dc7-a944-0317610cf359",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "config.share",
      "label": "Delta Share Configuration File Path",
      "name": "delta_share_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "config.share",
      "label": "Delta Share Configuration File Path",
      "name": "delta_share_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_name": {
     "currentValue": "scp-demo",
     "nuid": "81983825-df19-4d45-a8d9-1fe7180f6658",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "scp-demo",
      "label": "Delta Share Name",
      "name": "delta_share_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "scp-demo",
      "label": "Delta Share Name",
      "name": "delta_share_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "delta_share_provider": {
     "currentValue": "azure:eastus2:databricks:field-eng-east",
     "nuid": "38674dda-95fd-4d70-a0af-baa9b67a4d2a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "azure:eastus2:databricks:field-eng-east",
      "label": "Delta Share Provider Identifier",
      "name": "delta_share_provider",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "azure:eastus2:databricks:field-eng-east",
      "label": "Delta Share Provider Identifier",
      "name": "delta_share_provider",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "delta_share_schema": {
     "currentValue": "shared_schema",
     "nuid": "672ed729-5acb-40a7-9e93-40abc0d7b8fd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "shared_schema",
      "label": "Delta Share Schema Name",
      "name": "delta_share_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "shared_schema",
      "label": "Delta Share Schema Name",
      "name": "delta_share_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "delta_share_volume": {
     "currentValue": "shared_volume",
     "nuid": "bcb445b3-fd7d-4464-b159-5157bf1eebc1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "shared_volume",
      "label": "Delta Share Volume Name",
      "name": "delta_share_volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "shared_volume",
      "label": "Delta Share Volume Name",
      "name": "delta_share_volume",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "file_glob_pattern": {
     "currentValue": "*.parquet",
     "nuid": "9e7876e1-412c-4181-9f02-7ed89022d8e3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "*.parquet",
      "label": "File Glob Pattern to Copy",
      "name": "file_glob_pattern",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "*.parquet",
      "label": "File Glob Pattern to Copy",
      "name": "file_glob_pattern",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "3620fa84-451c-41c6-a614-aa9e93b65a69",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_data_path": {
     "currentValue": "/databricks-datasets/sample_data",
     "nuid": "a15850f1-ec5a-4b99-b18d-af9d807eed47",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/databricks-datasets/sample_data",
      "label": "Source Data Path to Copy to Volumes",
      "name": "source_data_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/databricks-datasets/sample_data",
      "label": "Source Data Path to Copy to Volumes",
      "name": "source_data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "user_list": {
     "currentValue": "marcin.jimenez@databricks.com",
     "nuid": "e04eab3a-a207-42e1-ace2-8c00475b2e2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated emails)",
      "name": "user_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated emails)",
      "name": "user_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "user_data_volume",
     "nuid": "e2f96381-8d4e-4670-b45d-37a702056b4f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
