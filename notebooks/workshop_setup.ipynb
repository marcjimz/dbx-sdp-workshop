{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aebc11c-297b-4fdf-9d03-1f89496ce380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop Setup - User Provisioning and Catalog Management\n",
    "\n",
    "This notebook automates the provisioning of Unity Catalog resources for multiple users.\n",
    "\n",
    "## Prerequisites\n",
    "- Delta Share must be created and shared in Databricks BEFORE running this notebook\n",
    "- You need the provider name and share name from your Delta Share setup\n",
    "\n",
    "## Workflow\n",
    "1. Load configuration from config.yaml\n",
    "2. Parse user list and generate user aliases\n",
    "3. Mount Delta Share to local catalog (format: `provider.share`)\n",
    "4. Create user-specific catalogs with naming convention\n",
    "5. Assign CAN MANAGE permissions to catalog owners\n",
    "6. Create volumes with consistent naming\n",
    "7. Load data from Delta Share volume to user volumes\n",
    "8. Generate provisioning report (CSV + JSON)\n",
    "\n",
    "## Configuration Pattern\n",
    "- Config values loaded from config.yaml\n",
    "- Widget-based parameter override capability\n",
    "- Type-safe parameter handling\n",
    "- Clear execution flow with status reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a49c21a-2621-46f0-bf29-e2ffbaee3822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Install required packages from requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de9d22d-c858-4ca1-9a70-4e2d98adca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdd0ddf-3c8f-42f8-a62d-3d568a952fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Configuration from config.yaml\n",
    "\n",
    "Load configuration file and use values as widget defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d4b5d9-6580-4fe4-818e-ecbe0f80cd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "# Support multiple execution contexts (local, workspace, etc.)\n",
    "config_paths = [\n",
    "    \"../config.yaml\",  # Relative path from notebooks/ directory\n",
    "    # \"/Workspace/Repos/dbx-sdp-workshop/config.yaml\",  # Workspace path\n",
    "    # \"config.yaml\"  # Current directory fallback\n",
    "]\n",
    "\n",
    "config = None\n",
    "config_loaded_from = None\n",
    "\n",
    "for config_path in config_paths:\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            config_loaded_from = config_path\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading config from {config_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if config is None:\n",
    "    print(\"‚ö†Ô∏è  Warning: Could not load config.yaml, using default values\")\n",
    "    config = {}\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_loaded_from}\")\n",
    "    print(f\"   Configuration keys: {list(config.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba40ddd-87e0-4f02-94b0-78bde3181e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Configuration Widgets\n",
    "\n",
    "Create widgets with defaults from config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61cbcf2-3f24-41fb-9876-eec2ac8c4289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widget Configuration\n",
    "# Config.yaml provides default values for all widgets\n",
    "\n",
    "# User Configuration\n",
    "dbutils.widgets.text(\"user_list\", config.get(\"user_list\", \"marcin.jimenez@databricks.com\"), \"User List (comma-separated emails)\")\n",
    "\n",
    "# Delta Share Configuration (Databricks-to-Databricks)\n",
    "# NOTE: Delta Share must be created and shared BEFORE running this notebook\n",
    "# Provider format: Use EXACT name from \"SHOW PROVIDERS;\" (e.g., \"azure:eastus2:databricks:field-eng-east\")\n",
    "dbutils.widgets.text(\"delta_share_provider\", config.get(\"delta_share_provider\", \"azure:eastus2:databricks:field-eng-east\"), \"Delta Share Provider (from SHOW PROVIDERS)\")\n",
    "dbutils.widgets.text(\"delta_share_name\", config.get(\"delta_share_name\", \"scp-demo\"), \"Delta Share Name (from SHOW SHARES)\")\n",
    "dbutils.widgets.text(\"delta_share_catalog\", config.get(\"delta_share_catalog\", \"shared_catalog\"), \"Delta Share Catalog Name (local)\")\n",
    "dbutils.widgets.text(\"delta_share_schema\", config.get(\"delta_share_schema\", \"shared_schema\"), \"Delta Share Schema Name\")\n",
    "dbutils.widgets.text(\"delta_share_volume\", config.get(\"delta_share_volume\", \"shared_volume\"), \"Delta Share Volume Name\")\n",
    "\n",
    "# File Pattern Configuration\n",
    "dbutils.widgets.text(\"file_glob_pattern\", config.get(\"file_glob_pattern\", \"*\"), \"File Glob Pattern to Copy\")\n",
    "\n",
    "# Catalog Configuration\n",
    "dbutils.widgets.text(\"base_catalog_name\", config.get(\"base_catalog_name\", \"workshop_catalog\"), \"Base Catalog Name\")\n",
    "\n",
    "# Volume Configuration\n",
    "dbutils.widgets.text(\"volume_name\", config.get(\"volume_name\", \"user_data_volume\"), \"Volume Name (consistent across all catalogs)\")\n",
    "dbutils.widgets.text(\"schema_name\", config.get(\"schema_name\", \"default\"), \"Schema Name for Volumes\")\n",
    "\n",
    "print(\"‚úÖ Configuration widgets created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530f30b5-e9a3-4463-b1cc-f1c82a1fe52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load and Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9500d3-fb07-4dd8-b103-3eee686ff7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve widget values (widgets override config.yaml if changed by user)\n",
    "user_list_raw = dbutils.widgets.get(\"user_list\")\n",
    "delta_share_provider = dbutils.widgets.get(\"delta_share_provider\")\n",
    "delta_share_name = dbutils.widgets.get(\"delta_share_name\")\n",
    "delta_share_catalog = dbutils.widgets.get(\"delta_share_catalog\")\n",
    "delta_share_schema = dbutils.widgets.get(\"delta_share_schema\")\n",
    "delta_share_volume = dbutils.widgets.get(\"delta_share_volume\")\n",
    "file_glob_pattern = dbutils.widgets.get(\"file_glob_pattern\")\n",
    "base_catalog_name = dbutils.widgets.get(\"base_catalog_name\")\n",
    "volume_name = dbutils.widgets.get(\"volume_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"üîß Workshop Provisioning Configuration:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Config Source:       {config_loaded_from or 'defaults'}\")\n",
    "print(f\"User List:           {user_list_raw}\")\n",
    "print(f\"Delta Share:         {delta_share_provider}.{delta_share_name}\")\n",
    "print(f\"Delta Share Catalog: {delta_share_catalog} (mounting to local workspace)\")\n",
    "print(f\"Delta Share Source:  {delta_share_catalog}.{delta_share_schema}.{delta_share_volume}\")\n",
    "print(f\"File Glob Pattern:   {file_glob_pattern}\")\n",
    "print(f\"Base Catalog:        {base_catalog_name}\")\n",
    "print(f\"Volume Name:         {volume_name}\")\n",
    "print(f\"Schema Name:         {schema_name}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723485eb-8128-4e1f-89f3-9561058f5206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Parse Users and Generate Aliases\n",
    "\n",
    "Parse the comma-separated user list and generate user aliases using the pattern:\n",
    "- First 3 letters of first name\n",
    "- First 4 letters of last name\n",
    "- Concatenated with underscore\n",
    "- Example: \"John Smith\" ‚Üí \"joh_smit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7401b9-00d7-4a4d-9324-cca09ce314a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_user_alias(email):\n",
    "    \"\"\"\n",
    "    Generate user alias from email address.\n",
    "    Pattern: first 3 letters of first name + _ + first 4 letters of last name\n",
    "    \n",
    "    Args:\n",
    "        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n",
    "    \n",
    "    Returns:\n",
    "        User alias (e.g., \"marjime\")\n",
    "    \"\"\"\n",
    "    # Extract username part from email (before @)\n",
    "    if '@' not in email:\n",
    "        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com'\")\n",
    "    \n",
    "    username = email.split('@')[0]\n",
    "    \n",
    "    # Split by dot or underscore to get first and last name\n",
    "    name_parts = re.split(r'[._]', username)\n",
    "    \n",
    "    if len(name_parts) < 2:\n",
    "        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com' or 'first_last@domain.com'\")\n",
    "    \n",
    "    first_name = name_parts[0].lower()\n",
    "    last_name = name_parts[-1].lower()  # Use last part in case of middle names\n",
    "    \n",
    "    # Generate alias: first 3 chars of first name + first 4 chars of last name\n",
    "    first_part = first_name[:3]\n",
    "    last_part = last_name[:4]\n",
    "    \n",
    "    alias = f\"{first_part}{last_part}\"\n",
    "    \n",
    "    # Remove any non-alphanumeric characters except underscore\n",
    "    alias = re.sub(r'[^a-z0-9_]', '', alias)\n",
    "    \n",
    "    return alias\n",
    "\n",
    "def extract_full_name(email):\n",
    "    \"\"\"\n",
    "    Extract full name from email address.\n",
    "    \n",
    "    Args:\n",
    "        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n",
    "    \n",
    "    Returns:\n",
    "        Full name with proper capitalization (e.g., \"Marcin Jimenez\")\n",
    "    \"\"\"\n",
    "    username = email.split('@')[0]\n",
    "    name_parts = re.split(r'[._]', username)\n",
    "    \n",
    "    # Capitalize each part\n",
    "    full_name = ' '.join(part.capitalize() for part in name_parts)\n",
    "    \n",
    "    return full_name\n",
    "\n",
    "# Parse user list (expecting comma-separated email addresses)\n",
    "users = [user.strip() for user in user_list_raw.split(',') if user.strip()]\n",
    "\n",
    "# Validate and generate user data structure\n",
    "user_data = []\n",
    "errors = []\n",
    "\n",
    "for email in users:\n",
    "    try:\n",
    "        alias = generate_user_alias(email)\n",
    "        full_name = extract_full_name(email)\n",
    "        \n",
    "        user_data.append({\n",
    "            \"full_name\": full_name,\n",
    "            \"alias\": alias,\n",
    "            \"catalog_name\": f\"{base_catalog_name}_{alias}\",\n",
    "            \"email\": email\n",
    "        })\n",
    "    except ValueError as e:\n",
    "        errors.append(str(e))\n",
    "\n",
    "# If there are any errors, raise an exception\n",
    "if errors:\n",
    "    error_message = \"‚ùå User parsing failed:\\n\" + \"\\n\".join(f\"   - {err}\" for err in errors)\n",
    "    raise ValueError(error_message)\n",
    "\n",
    "# Display parsed users\n",
    "print(f\"\\nüìã Parsed {len(user_data)} users:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, user in enumerate(user_data, 1):\n",
    "    print(f\"{idx}. {user['email']:40s} ‚Üí {user['full_name']:20s} ‚Üí Alias: {user['alias']:12s} ‚Üí Catalog: {user['catalog_name']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57691b7-3d5c-4b6a-8f29-8cfbbbf97587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Mount Delta Share to Catalog\n",
    "\n",
    "Mount the Delta Share to a local catalog for accessing shared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2de51a-93bf-4cf0-bd6b-60647699a10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "def mount_delta_share_catalog(catalog_name, provider, share_name):\n",
    "    try:\n",
    "        # Check if catalog already exists\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE CATALOG `{catalog_name}`\")\n",
    "            print(f\"‚úÖ Delta Share catalog '{catalog_name}' already mounted\")\n",
    "            return {\"status\": \"already_exists\", \"mounted\": True, \"catalog\": catalog_name}\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "\n",
    "        # Create catalog from Delta Share\n",
    "        catalog = w.catalogs.create(\n",
    "            name=catalog_name,\n",
    "            provider_name=provider,\n",
    "            share_name=share_name,\n",
    "            comment=\"Catalog created for SCP Workshop\"  # optional\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Mounted Delta Share '{share_name}' to catalog '{catalog_name}'\")\n",
    "        return {\"status\": \"success\", \"mounted\": True, \"catalog\": catalog_name}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mounting Delta Share: {str(e)}\")\n",
    "        print(f\"   Verify share exists: {provider}.{share_name}\")\n",
    "        print(f\"   Run 'SHOW PROVIDERS;' and 'SHOW SHARES;' to verify names\")\n",
    "        raise e\n",
    "\n",
    "# Mount Delta Share\n",
    "print(\"\\nüì¶ Mounting Delta Share to Catalog:\")\n",
    "print(\"=\" * 80)\n",
    "mount_status = mount_delta_share_catalog(delta_share_catalog, delta_share_provider, delta_share_name)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c7f5a1-a0ad-4461-8ac0-13686774d1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Create User Catalogs\n",
    "\n",
    "Create Unity Catalog catalogs for each user with the naming pattern: `{base_catalog_name}_{user_alias}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a2983e5-bf8c-4059-b7bf-5674d2376204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_catalog(catalog_name, comment):\n",
    "    \"\"\"Create a Unity Catalog catalog.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog_name}` COMMENT '{comment}'\")\n",
    "        return {\"catalog\": catalog_name, \"created\": True, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating catalog {catalog_name}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Create catalogs for all users\n",
    "print(\"\\nüìö Creating User Catalogs:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "catalog_results = []\n",
    "for user in user_data:\n",
    "    comment = f\"Catalog for user {user['full_name']} created: {user['catalog_name']}\"\n",
    "    result = create_catalog(user['catalog_name'], comment)\n",
    "    catalog_results.append(result)\n",
    "    user['catalog_created'] = result['created']\n",
    "    print(comment)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in catalog_results if r.get('created', False))\n",
    "print(f\"üìä Created {success_count}/{len(catalog_results)} catalogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2e3c02-5eb5-4baa-8971-7e1f61f3de30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Assign CAN MANAGE Permissions\n",
    "\n",
    "Grant CAN MANAGE permissions to each user for their respective catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a38e08-c8cd-4bfe-925f-f84163320626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def grant_catalog_permissions(catalog_name, user_email):\n",
    "    \"\"\"Grant CAN MANAGE permissions on a catalog to a user.\"\"\"\n",
    "    try:\n",
    "        for priv in [\"USE CATALOG\", \"USE SCHEMA\", \"CREATE SCHEMA\"]:\n",
    "            spark.sql(f\"GRANT {priv} ON CATALOG `{catalog_name}` TO `{user_email}`\")\n",
    "        \n",
    "        spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG `{catalog_name}` TO `{user_email}`\")\n",
    "        print(f\"Successful granting permissions on {catalog_name} to {user_email}\")\n",
    "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": True, \"status\": \"success\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error granting permissions on {catalog_name} to {user_email}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Grant permissions for all users\n",
    "print(\"\\nüîê Assigning CAN MANAGE Permissions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "permission_results = []\n",
    "for user in user_data:\n",
    "    result = grant_catalog_permissions(user['catalog_name'], user['email'])\n",
    "    permission_results.append(result)\n",
    "    user['permissions_granted'] = result['granted']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in permission_results if r.get('granted', False))\n",
    "print(f\"üìä Granted permissions to {success_count}/{len(permission_results)} catalogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9f32c8-502a-4477-97ae-b2a91a6cb240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Create Volumes with Consistent Naming\n",
    "\n",
    "Create a volume in each catalog with a consistent name across all catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c14fbb1f-44d4-4e61-918d-5a1b4c120cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema_and_volume(catalog_name, schema_name, volume_name):\n",
    "    \"\"\"Create schema and volume in a catalog.\"\"\"\n",
    "    try:\n",
    "        volume_path = f\"{catalog_name}.{schema_name}.{volume_name}\"\n",
    "        \n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.`{schema_name}` COMMENT 'Schema for user data and volumes'\")\n",
    "        spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`{volume_name}` COMMENT 'User data volume'\")\n",
    "        print(f\"Successful creating volume in {catalog_name}.{schema_name}.{volume_name}\")\n",
    "        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": True, \"status\": \"success\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating volume in {catalog_name}: {str(e)}\")\n",
    "        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Create volumes for all users\n",
    "print(\"\\nüíæ Creating User Volumes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "volume_results = []\n",
    "for user in user_data:\n",
    "    result = create_schema_and_volume(user['catalog_name'], schema_name, volume_name)\n",
    "    volume_results.append(result)\n",
    "    user['volume_name'] = result['volume_path']\n",
    "    user['volume_created'] = result['created']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "success_count = sum(1 for r in volume_results if r.get('created', False))\n",
    "print(f\"üìä Created {success_count}/{len(volume_results)} volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c6ca7a-b2f4-422d-b0a9-4c0b29ac8ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Load Data to Volumes from Delta Share\n",
    "\n",
    "Copy data from Delta Share volume to each user's volume using the glob pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f74b13-d6e6-4683-887f-30316693f51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "def list_files_recursive(path):\n",
    "    \"\"\"Recursively list all files in a path.\"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    def traverse(current_path):\n",
    "        try:\n",
    "            items = dbutils.fs.ls(current_path)\n",
    "            for item in items:\n",
    "                if item.isDir():\n",
    "                    # Recursively traverse directories\n",
    "                    traverse(item.path)\n",
    "                else:\n",
    "                    # Add files to the list\n",
    "                    all_files.append(item)\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è  Error accessing {current_path}: {str(e)}\")\n",
    "    \n",
    "    traverse(path)\n",
    "    return all_files\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"Remove dbfs: prefix and normalize path.\"\"\"\n",
    "    return path.replace('dbfs:', '').replace('//', '/')\n",
    "\n",
    "def load_data_to_volume(volume_path, delta_share_catalog, delta_share_schema, delta_share_volume, file_pattern):\n",
    "    \"\"\"Copy data from Delta Share volume to user volume using glob pattern.\"\"\"\n",
    "    try:\n",
    "        # Root destination - no /data subdirectory\n",
    "        volume_fs_path = f\"/Volumes/{volume_path.replace('.', '/')}\"\n",
    "        \n",
    "        # Access Delta Share volume\n",
    "        delta_share_volume_path = f\"/Volumes/{delta_share_catalog}/{delta_share_schema}/{delta_share_volume}\"\n",
    "        \n",
    "        print(f\"   üìÅ Source: {delta_share_volume_path}\")\n",
    "        print(f\"   üìÅ Destination: {volume_fs_path}\")\n",
    "        print(f\"   üîç Pattern: {file_pattern}\")\n",
    "        \n",
    "        # List and filter files recursively\n",
    "        try:\n",
    "            print(f\"   üîÑ Scanning directories recursively...\")\n",
    "            all_files = list_files_recursive(delta_share_volume_path)\n",
    "            print(f\"   üìã Found {len(all_files)} files in source (recursive)\")\n",
    "            \n",
    "            # Debug: Show all files found\n",
    "            for f in all_files:\n",
    "                print(f\"      - {f.name} (matches pattern: {fnmatch.fnmatch(f.name, file_pattern)})\")\n",
    "            \n",
    "            matching_files = [f for f in all_files if fnmatch.fnmatch(f.name, file_pattern)]\n",
    "            \n",
    "            if not matching_files:\n",
    "                print(f\"   ‚ö†Ô∏è  No files match pattern '{file_pattern}'\")\n",
    "                return {\"volume\": volume_path, \"data_location\": volume_fs_path, \"loaded\": False, \"status\": \"no_files_found\", \"files_copied\": 0}\n",
    "            \n",
    "            print(f\"   ‚úì {len(matching_files)} files match pattern\")\n",
    "            \n",
    "            # Copy files preserving directory structure\n",
    "            files_copied = 0\n",
    "            errors = []\n",
    "            for file_obj in matching_files:\n",
    "                # Normalize paths to remove dbfs: prefix\n",
    "                normalized_file_path = normalize_path(file_obj.path)\n",
    "                normalized_source_path = normalize_path(delta_share_volume_path)\n",
    "                \n",
    "                # Get relative path from source\n",
    "                relative_path = normalized_file_path.replace(normalized_source_path, '').lstrip('/')\n",
    "                dest_path = f\"{volume_fs_path}/{relative_path}\"\n",
    "                \n",
    "                # print(f\"      üîç Debug: {normalized_file_path} -> {dest_path}\")\n",
    "                \n",
    "                # Create destination directory if needed\n",
    "                dest_dir = '/'.join(dest_path.split('/')[:-1])\n",
    "                try:\n",
    "                    dbutils.fs.mkdirs(dest_dir)\n",
    "                except:\n",
    "                    pass  # Directory might already exist\n",
    "                \n",
    "                try:\n",
    "                    dbutils.fs.cp(file_obj.path, dest_path)\n",
    "                    files_copied += 1\n",
    "                    print(f\"      ‚úì Copied: {relative_path}\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to copy {relative_path}: {str(e)}\"\n",
    "                    errors.append(error_msg)\n",
    "                    print(f\"      ‚úó {error_msg}\")\n",
    "            \n",
    "            if errors:\n",
    "                print(f\"   ‚ö†Ô∏è  Encountered {len(errors)} errors during copy\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully loaded data to {volume_path} ({files_copied} files)\")\n",
    "            \n",
    "            return {\n",
    "                \"volume\": volume_path,\n",
    "                \"data_location\": volume_fs_path,\n",
    "                \"loaded\": True if files_copied > 0 else False,\n",
    "                \"status\": \"success\" if files_copied > 0 else \"copy_errors\",\n",
    "                \"files_copied\": files_copied,\n",
    "                \"source\": \"delta_share\",\n",
    "                \"errors\": errors if errors else None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error listing files: {str(e)}\")\n",
    "            raise e\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data to {volume_path}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Load data to all volumes\n",
    "print(\"\\nüì• Loading Data to Volumes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_load_results = []\n",
    "for user in user_data:\n",
    "    print(f\"\\nüë§ Processing user: {user['full_name']} ({user['alias']})\")\n",
    "    \n",
    "    if not user.get('volume_created', False):\n",
    "        print(f\"   ‚ö†Ô∏è  Skipping - volume was not created successfully\")\n",
    "        user['data_location'] = None\n",
    "        user['data_loaded'] = False\n",
    "        user['files_copied'] = 0\n",
    "        continue\n",
    "    \n",
    "    result = load_data_to_volume(\n",
    "        user['volume_name'],\n",
    "        delta_share_catalog,\n",
    "        delta_share_schema,\n",
    "        delta_share_volume,\n",
    "        file_glob_pattern\n",
    "    )\n",
    "    data_load_results.append(result)\n",
    "    user['data_location'] = result['data_location']\n",
    "    user['data_loaded'] = result['loaded']\n",
    "    user['files_copied'] = result.get('files_copied', 0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "success_count = sum(1 for r in data_load_results if r.get('loaded', False))\n",
    "total_files = sum(r.get('files_copied', 0) for r in data_load_results)\n",
    "\n",
    "print(f\"üìä Loaded data to {success_count}/{len(data_load_results)} volumes\")\n",
    "print(f\"   Total files copied: {total_files}\")\n",
    "\n",
    "# Show any errors\n",
    "errors_found = [r for r in data_load_results if r.get('errors')]\n",
    "if errors_found:\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(errors_found)} volumes had copy errors - see details above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4152983a-d5ab-4e39-9042-e3886b8ca145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Generate Provisioning Report Table\n",
    "\n",
    "Create a comprehensive report table showing all provisioning details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b259d2f7-8fde-45aa-b7ee-b5c02b8900bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "# Define schema for the report\n",
    "report_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), False),\n",
    "    StructField(\"user_alias\", StringType(), False),\n",
    "    StructField(\"catalog_created\", StringType(), False),\n",
    "    StructField(\"permissions_assigned\", BooleanType(), False),\n",
    "    StructField(\"volume_name\", StringType(), True),\n",
    "    StructField(\"volume_data_location\", StringType(), True),\n",
    "    StructField(\"provisioning_status\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Build report rows\n",
    "report_rows = []\n",
    "for idx, user in enumerate(user_data, 1):\n",
    "    # Determine overall provisioning status\n",
    "    if user.get('catalog_created', False) and user.get('permissions_granted', False) and user.get('volume_created', False) and user.get('data_loaded', False):\n",
    "        status = \"‚úÖ Complete\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è Partial\"\n",
    "    \n",
    "    report_rows.append(\n",
    "        Row(\n",
    "            user_id=user['email'],\n",
    "            full_name=user['full_name'],\n",
    "            user_alias=user['alias'],\n",
    "            catalog_created=user['catalog_name'],\n",
    "            permissions_assigned=user.get('permissions_granted', False),\n",
    "            volume_name=user.get('volume_name', None),\n",
    "            volume_data_location=user.get('data_location', None),\n",
    "            provisioning_status=status\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create DataFrame\n",
    "report_df = spark.createDataFrame(report_rows, schema=report_schema)\n",
    "\n",
    "# Display report\n",
    "print(\"\\nüìä Provisioning Report:\")\n",
    "print(\"=\" * 80)\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d52103-2c27-4b8d-91a3-3288cb1d5ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5894966d-453e-49a0-9c94-d6566463dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_users = len(user_data)\n",
    "catalogs_created = sum(1 for u in user_data if u.get('catalog_created', False))\n",
    "permissions_granted = sum(1 for u in user_data if u.get('permissions_granted', False))\n",
    "volumes_created = sum(1 for u in user_data if u.get('volume_created', False))\n",
    "data_loaded = sum(1 for u in user_data if u.get('data_loaded', False))\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà WORKSHOP PROVISIONING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Users Processed:        {total_users}\")\n",
    "print(f\"Catalogs Created:             {catalogs_created}/{total_users} ({catalogs_created/total_users*100:.1f}%)\")\n",
    "print(f\"Permissions Granted:          {permissions_granted}/{total_users} ({permissions_granted/total_users*100:.1f}%)\")\n",
    "print(f\"Volumes Created:              {volumes_created}/{total_users} ({volumes_created/total_users*100:.1f}%)\")\n",
    "print(f\"Data Loaded:                  {data_loaded}/{total_users} ({data_loaded/total_users*100:.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Success check\n",
    "if catalogs_created == total_users and permissions_granted == total_users and volumes_created == total_users and data_loaded == total_users:\n",
    "    print(\"\\n‚úÖ SUCCESS: All users provisioned successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Some provisioning steps failed. Review the report above for details.\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84285cd6-60c2-49b7-ae05-02fc90529b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Export Report to CSV\n",
    "\n",
    "Save the provisioning report to a CSV file for tracking and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237e04be-26ce-4052-9e5f-f75c0c335f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save report to CSV and JSON for cleanup tracking\n",
    "report_csv_file = \"../provisioning_report.csv\"\n",
    "report_json_file = \"../provisioning_report.json\"\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Save JSON report for cleanup operations\n",
    "    report_data = {\n",
    "        \"base_catalog_name\": base_catalog_name,\n",
    "        \"users\": user_data,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(report_json_file, 'w') as f:\n",
    "        json.dump(report_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ JSON report saved to: {report_json_file}\")\n",
    "    \n",
    "    # Save CSV report for easy viewing\n",
    "    report_df.toPandas().to_csv(report_csv_file, index=False)\n",
    "    print(f\"üíæ CSV report saved to: {report_csv_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not save reports: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706b567b-749f-4c21-9edc-527732ac3660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup Section - Delete Provisioned Resources\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: The following cells will DELETE all catalogs created by this workshop setup.\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the first cell to load the provisioning report\n",
    "2. Run the second cell and type \"CONFIRM\" in the widget to delete all resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bedfc8dc-264b-43ed-9ff1-8dae18b3223a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Try to load the provisioning report\n",
    "report_file_paths = [\n",
    "    \"../provisioning_report.json\",\n",
    "    \"/Workspace/Repos/dbx-sdp-workshop/provisioning_report.json\",\n",
    "    \"provisioning_report.json\"\n",
    "]\n",
    "\n",
    "cleanup_data = None\n",
    "report_file_loaded = None\n",
    "\n",
    "for path in report_file_paths:\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            cleanup_data = json.load(f)\n",
    "            report_file_loaded = path\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading {path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if cleanup_data is None:\n",
    "    print(\"‚ùå No provisioning report found!\")\n",
    "    print(\"   Run the provisioning cells above first to create resources.\")\n",
    "    print(\"   The report file (provisioning_report.json) is required for cleanup.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Provisioning report loaded from: {report_file_loaded}\")\n",
    "    print(f\"   Base catalog name: {cleanup_data.get('base_catalog_name')}\")\n",
    "    print(f\"   Users found: {len(cleanup_data.get('users', []))}\")\n",
    "    print(f\"   Timestamp: {cleanup_data.get('timestamp', 'N/A')}\")\n",
    "    \n",
    "    # Display catalogs to be deleted\n",
    "    print(f\"\\nüìã Catalogs that will be deleted:\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, user in enumerate(cleanup_data.get('users', []), 1):\n",
    "        print(f\"{idx}. {user['catalog_name']}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e749caf-4bb7-407f-9ca0-28c207ce7f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create confirmation widget if it doesn't exist\n",
    "\n",
    "# deletion_results = []\n",
    "\n",
    "# for user in cleanup_data.get('users', []):\n",
    "#     catalog_name = user.get('catalog_name')\n",
    "    \n",
    "#     try:\n",
    "#         # Drop the catalog (CASCADE will delete all contents)\n",
    "#         spark.sql(f\"DROP CATALOG IF EXISTS `{catalog_name}` CASCADE\")\n",
    "#         print(f\"   ‚úÖ Deleted catalog: {catalog_name}\")\n",
    "#         deletion_results.append({\n",
    "#             \"catalog\": catalog_name,\n",
    "#             \"deleted\": True,\n",
    "#             \"status\": \"success\"\n",
    "#         })\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ùå Error deleting catalog {catalog_name}: {str(e)}\")\n",
    "#         deletion_results.append({\n",
    "#             \"catalog\": catalog_name,\n",
    "#             \"deleted\": False,\n",
    "#             \"status\": \"error\",\n",
    "#             \"error\": str(e)\n",
    "#         })\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # Display summary\n",
    "# success_count = sum(1 for r in deletion_results if r.get('deleted', False))\n",
    "# total_count = len(deletion_results)\n",
    "\n",
    "# print(f\"\\nüìä Deletion Summary:\")\n",
    "# print(f\"   Catalogs deleted: {success_count}/{total_count}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "auto-run-selected-command"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6210960758497821,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "base_catalog_name",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "delta_share_catalog",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "delta_share_name",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "delta_share_provider",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "delta_share_schema",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "delta_share_volume",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "file_glob_pattern",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "schema_name",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "source_data_path",
      "width": 206
     },
     {
      "breakBefore": false,
      "name": "user_list",
      "width": 206
     }
    ]
   },
   "notebookName": "workshop_setup",
   "widgets": {
    "base_catalog_name": {
     "currentValue": "databricks_scp_workshop",
     "nuid": "43589a2c-5e5e-4edf-8b2b-8ca45de311f6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks_scp_workshop",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks_scp_workshop",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_catalog": {
     "currentValue": "dbx_scp_shared_catalog",
     "nuid": "70ed3413-ec66-4ffe-967e-4b03afc02b9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbx_scp_shared_catalog",
      "label": "Delta Share Catalog Name (local)",
      "name": "delta_share_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbx_scp_shared_catalog",
      "label": "Delta Share Catalog Name (local)",
      "name": "delta_share_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_name": {
     "currentValue": "scp-demo",
     "nuid": "4c5cdf58-48a3-419a-96f9-fdb37e256ff5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "scp-demo",
      "label": "Delta Share Name (from SHOW SHARES)",
      "name": "delta_share_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "scp-demo",
      "label": "Delta Share Name (from SHOW SHARES)",
      "name": "delta_share_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_provider": {
     "currentValue": "azure:eastus2:databricks:field-eng-east",
     "nuid": "5c0417e5-b5ad-4c12-bf0c-b9b1c8390513",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "azure:eastus2:databricks:field-eng-east",
      "label": "Delta Share Provider (from SHOW PROVIDERS)",
      "name": "delta_share_provider",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "azure:eastus2:databricks:field-eng-east",
      "label": "Delta Share Provider (from SHOW PROVIDERS)",
      "name": "delta_share_provider",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_schema": {
     "currentValue": "hedis_measurements",
     "nuid": "1244a7f0-e40c-435c-bdc3-5289f0e2cb85",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "hedis_measurements",
      "label": "Delta Share Schema Name",
      "name": "delta_share_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "hedis_measurements",
      "label": "Delta Share Schema Name",
      "name": "delta_share_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_volume": {
     "currentValue": "hedis",
     "nuid": "8dbf1abf-607a-4ab2-9dfe-001061373fe4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "hedis",
      "label": "Delta Share Volume Name",
      "name": "delta_share_volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "hedis",
      "label": "Delta Share Volume Name",
      "name": "delta_share_volume",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_glob_pattern": {
     "currentValue": "*.pdf",
     "nuid": "fd965b53-efb5-43fd-a616-d7e22482001b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "*",
      "label": "File Glob Pattern to Copy",
      "name": "file_glob_pattern",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "*",
      "label": "File Glob Pattern to Copy",
      "name": "file_glob_pattern",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "e9ca1509-26f7-4ca1-bab2-67b7b0da2e16",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "user_list": {
     "currentValue": "marcin.jimenez@databricks.com",
     "nuid": "63261eba-8254-4114-a265-830e382c7ede",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated emails)",
      "name": "user_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated emails)",
      "name": "user_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "user_data_volume",
     "nuid": "e70aab19-2047-44f8-8ffb-9024964357e9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
