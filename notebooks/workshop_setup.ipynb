{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aebc11c-297b-4fdf-9d03-1f89496ce380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop Setup - User Provisioning and Catalog Management\n",
    "\n",
    "This notebook automates the provisioning of Unity Catalog resources for multiple users.\n",
    "\n",
    "## Workflow\n",
    "1. Load configuration from config.yaml\n",
    "2. Parse user list and generate user aliases\n",
    "3. Configure Delta Share recipient\n",
    "4. Create user-specific catalogs with naming convention\n",
    "5. Assign CAN MANAGE permissions to catalog owners\n",
    "6. Create volumes with consistent naming\n",
    "7. Load data to volumes\n",
    "8. Generate provisioning report table\n",
    "\n",
    "## Configuration Pattern\n",
    "- Config values loaded from config.yaml\n",
    "- Widget-based parameter override capability\n",
    "- Type-safe parameter handling\n",
    "- Clear execution flow with status reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a49c21a-2621-46f0-bf29-e2ffbaee3822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Install required packages from requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de9d22d-c858-4ca1-9a70-4e2d98adca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffdd0ddf-3c8f-42f8-a62d-3d568a952fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Configuration from config.yaml\n",
    "\n",
    "Load configuration file and use values as widget defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d4b5d9-6580-4fe4-818e-ecbe0f80cd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "# Support multiple execution contexts (local, workspace, etc.)\n",
    "config_paths = [\n",
    "    \"../config.yaml\",  # Relative path from notebooks/ directory\n",
    "    # \"/Workspace/Repos/dbx-sdp-workshop/config.yaml\",  # Workspace path\n",
    "    # \"config.yaml\"  # Current directory fallback\n",
    "]\n",
    "\n",
    "config = None\n",
    "config_loaded_from = None\n",
    "\n",
    "for config_path in config_paths:\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            config_loaded_from = config_path\n",
    "            break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading config from {config_path}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if config is None:\n",
    "    print(\"‚ö†Ô∏è  Warning: Could not load config.yaml, using default values\")\n",
    "    config = {}\n",
    "else:\n",
    "    print(f\"‚úÖ Configuration loaded from: {config_loaded_from}\")\n",
    "    print(f\"   Configuration keys: {list(config.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba40ddd-87e0-4f02-94b0-78bde3181e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Configuration Widgets\n",
    "\n",
    "Create widgets with defaults from config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61cbcf2-3f24-41fb-9876-eec2ac8c4289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Widget Configuration\n# Config.yaml provides default values for all widgets\n\n# User Configuration\ndbutils.widgets.text(\"user_list\", config.get(\"user_list\", \"marcin.jimenez@databricks.com\"), \"User List (comma-separated emails)\")\n\n# Delta Share Configuration\ndbutils.widgets.text(\"delta_share_file\", config.get(\"delta_share_file\", \"config.share\"), \"Delta Share Configuration File Path\")\ndbutils.widgets.text(\"delta_share_provider\", config.get(\"delta_share_provider\", \"azure:eastus2:databricks:field-eng-east\"), \"Delta Share Provider Identifier\")\ndbutils.widgets.text(\"delta_share_name\", config.get(\"delta_share_name\", \"scp-demo\"), \"Delta Share Name\")\ndbutils.widgets.text(\"delta_share_catalog\", config.get(\"delta_share_catalog\", \"shared_catalog\"), \"Delta Share Catalog Name (local)\")\ndbutils.widgets.text(\"delta_share_schema\", config.get(\"delta_share_schema\", \"shared_schema\"), \"Delta Share Schema Name\")\ndbutils.widgets.text(\"delta_share_volume\", config.get(\"delta_share_volume\", \"shared_volume\"), \"Delta Share Volume Name\")\n\n# File Pattern Configuration\ndbutils.widgets.text(\"file_glob_pattern\", config.get(\"file_glob_pattern\", \"*.parquet\"), \"File Glob Pattern to Copy\")\n\n# Catalog Configuration\ndbutils.widgets.text(\"base_catalog_name\", config.get(\"base_catalog_name\", \"workshop_catalog\"), \"Base Catalog Name\")\n\n# Volume Configuration\ndbutils.widgets.text(\"volume_name\", config.get(\"volume_name\", \"user_data_volume\"), \"Volume Name (consistent across all catalogs)\")\ndbutils.widgets.text(\"schema_name\", config.get(\"schema_name\", \"default\"), \"Schema Name for Volumes\")\n\n# Data Source Configuration\ndbutils.widgets.text(\"source_data_path\", config.get(\"source_data_path\", \"/databricks-datasets/sample_data\"), \"Source Data Path to Copy to Volumes\")\n\nprint(\"‚úÖ Configuration widgets created successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530f30b5-e9a3-4463-b1cc-f1c82a1fe52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load and Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9500d3-fb07-4dd8-b103-3eee686ff7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Retrieve widget values (widgets override config.yaml if changed by user)\nuser_list_raw = dbutils.widgets.get(\"user_list\")\ndelta_share_file = dbutils.widgets.get(\"delta_share_file\")\ndelta_share_provider = dbutils.widgets.get(\"delta_share_provider\")\ndelta_share_name = dbutils.widgets.get(\"delta_share_name\")\ndelta_share_catalog = dbutils.widgets.get(\"delta_share_catalog\")\ndelta_share_schema = dbutils.widgets.get(\"delta_share_schema\")\ndelta_share_volume = dbutils.widgets.get(\"delta_share_volume\")\nfile_glob_pattern = dbutils.widgets.get(\"file_glob_pattern\")\nbase_catalog_name = dbutils.widgets.get(\"base_catalog_name\")\nvolume_name = dbutils.widgets.get(\"volume_name\")\nschema_name = dbutils.widgets.get(\"schema_name\")\nsource_data_path = dbutils.widgets.get(\"source_data_path\")\n\n# Display configuration\nprint(\"üîß Workshop Provisioning Configuration:\")\nprint(\"=\" * 80)\nprint(f\"Config Source:       {config_loaded_from or 'defaults'}\")\nprint(f\"User List:           {user_list_raw}\")\nprint(f\"Delta Share:         {delta_share_provider}.{delta_share_name}\")\nprint(f\"Delta Share Catalog: {delta_share_catalog} (mounting to local workspace)\")\nprint(f\"Delta Share Source:  {delta_share_catalog}.{delta_share_schema}.{delta_share_volume}\")\nprint(f\"File Glob Pattern:   {file_glob_pattern}\")\nprint(f\"Base Catalog:        {base_catalog_name}\")\nprint(f\"Volume Name:         {volume_name}\")\nprint(f\"Schema Name:         {schema_name}\")\nprint(f\"Fallback Source:     {source_data_path}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723485eb-8128-4e1f-89f3-9561058f5206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Parse Users and Generate Aliases\n",
    "\n",
    "Parse the comma-separated user list and generate user aliases using the pattern:\n",
    "- First 3 letters of first name\n",
    "- First 4 letters of last name\n",
    "- Concatenated with underscore\n",
    "- Example: \"John Smith\" ‚Üí \"joh_smit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7401b9-00d7-4a4d-9324-cca09ce314a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "import re\n\ndef generate_user_alias(email):\n    \"\"\"\n    Generate user alias from email address.\n    Pattern: first 3 letters of first name + _ + first 4 letters of last name\n    \n    Args:\n        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n    \n    Returns:\n        User alias (e.g., \"mar_jime\")\n    \"\"\"\n    # Extract username part from email (before @)\n    if '@' not in email:\n        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com'\")\n    \n    username = email.split('@')[0]\n    \n    # Split by dot or underscore to get first and last name\n    name_parts = re.split(r'[._]', username)\n    \n    if len(name_parts) < 2:\n        raise ValueError(f\"Invalid email format: {email}. Expected 'first.last@domain.com' or 'first_last@domain.com'\")\n    \n    first_name = name_parts[0].lower()\n    last_name = name_parts[-1].lower()  # Use last part in case of middle names\n    \n    # Generate alias: first 3 chars of first name + first 4 chars of last name\n    first_part = first_name[:3]\n    last_part = last_name[:4]\n    \n    alias = f\"{first_part}_{last_part}\"\n    \n    # Remove any non-alphanumeric characters except underscore\n    alias = re.sub(r'[^a-z0-9_]', '', alias)\n    \n    return alias\n\ndef extract_full_name(email):\n    \"\"\"\n    Extract full name from email address.\n    \n    Args:\n        email: Email address (e.g., \"marcin.jimenez@databricks.com\")\n    \n    Returns:\n        Full name with proper capitalization (e.g., \"Marcin Jimenez\")\n    \"\"\"\n    username = email.split('@')[0]\n    name_parts = re.split(r'[._]', username)\n    \n    # Capitalize each part\n    full_name = ' '.join(part.capitalize() for part in name_parts)\n    \n    return full_name\n\n# Parse user list (expecting comma-separated email addresses)\nusers = [user.strip() for user in user_list_raw.split(',') if user.strip()]\n\n# Validate and generate user data structure\nuser_data = []\nerrors = []\n\nfor email in users:\n    try:\n        alias = generate_user_alias(email)\n        full_name = extract_full_name(email)\n        \n        user_data.append({\n            \"full_name\": full_name,\n            \"alias\": alias,\n            \"catalog_name\": f\"{base_catalog_name}_{alias}\",\n            \"email\": email\n        })\n    except ValueError as e:\n        errors.append(str(e))\n\n# If there are any errors, raise an exception\nif errors:\n    error_message = \"‚ùå User parsing failed:\\n\" + \"\\n\".join(f\"   - {err}\" for err in errors)\n    raise ValueError(error_message)\n\n# Display parsed users\nprint(f\"\\nüìã Parsed {len(user_data)} users:\")\nprint(\"=\" * 80)\nfor idx, user in enumerate(user_data, 1):\n    print(f\"{idx}. {user['email']:40s} ‚Üí {user['full_name']:20s} ‚Üí Alias: {user['alias']:12s} ‚Üí Catalog: {user['catalog_name']}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2de51a-93bf-4cf0-bd6b-60647699a10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "def mount_delta_share_catalog(catalog_name, provider, share_name):\n    \"\"\"\n    Mount a Delta Share to a local catalog.\n    \n    Args:\n        catalog_name: Local catalog name to create\n        provider: Delta Share provider identifier (e.g., \"azure:eastus2:databricks:field-eng-east\")\n        share_name: Name of the share (e.g., \"scp-demo\")\n    \n    Returns:\n        Mount status dict\n    \"\"\"\n    try:\n        # Check if catalog already exists\n        try:\n            spark.sql(f\"DESCRIBE CATALOG `{catalog_name}`\")\n            print(f\"‚úÖ Delta Share catalog '{catalog_name}' already mounted\")\n            return {\"status\": \"already_exists\", \"mounted\": True, \"catalog\": catalog_name}\n        except Exception:\n            pass\n        \n        # Create catalog from Delta Share\n        share_path = f\"{provider}.{share_name}\"\n        spark.sql(f\"\"\"\n            CREATE CATALOG IF NOT EXISTS `{catalog_name}`\n            USING SHARE `{share_path}`\n        \"\"\")\n        \n        print(f\"‚úÖ Mounted Delta Share '{share_path}' to catalog '{catalog_name}'\")\n        return {\"status\": \"success\", \"mounted\": True, \"catalog\": catalog_name}\n        \n    except Exception as e:\n        print(f\"‚ùå Error mounting Delta Share: {str(e)}\")\n        print(f\"   Verify share exists: {provider}.{share_name}\")\n        return {\"status\": \"error\", \"mounted\": False, \"error\": str(e)}\n\n# Mount Delta Share\nprint(\"\\nüì¶ Mounting Delta Share to Catalog:\")\nprint(\"=\" * 80)\nmount_status = mount_delta_share_catalog(delta_share_catalog, delta_share_provider, delta_share_name)\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Mount Delta Share to Catalog\n\nMount the Delta Share to a local catalog for accessing shared data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Create User Catalogs\n\nCreate Unity Catalog catalogs for each user with the naming pattern: `{base_catalog_name}_{user_alias}`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a2983e5-bf8c-4059-b7bf-5674d2376204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "def create_catalog(catalog_name, comment):\n    \"\"\"Create a Unity Catalog catalog.\"\"\"\n    try:\n        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog_name}` COMMENT '{comment}'\")\n        return {\"catalog\": catalog_name, \"created\": True, \"status\": \"success\"}\n    except Exception as e:\n        print(f\"‚ùå Error creating catalog {catalog_name}: {str(e)}\")\n        return {\"catalog\": catalog_name, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n\n# Create catalogs for all users\nprint(\"\\nüìö Creating User Catalogs:\")\nprint(\"=\" * 80)\n\ncatalog_results = []\nfor user in user_data:\n    comment = f\"Catalog for user {user['full_name']} ({user['alias']})\"\n    result = create_catalog(user['catalog_name'], comment)\n    catalog_results.append(result)\n    user['catalog_created'] = result['created']\n\nprint(\"=\" * 80)\nsuccess_count = sum(1 for r in catalog_results if r.get('created', False))\nprint(f\"üìä Created {success_count}/{len(catalog_results)} catalogs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a38e08-c8cd-4bfe-925f-f84163320626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "def grant_catalog_permissions(catalog_name, user_email):\n    \"\"\"Grant CAN MANAGE permissions on a catalog to a user.\"\"\"\n    try:\n        for priv in [\"USE CATALOG\", \"USE SCHEMA\", \"CREATE SCHEMA\"]:\n            spark.sql(f\"GRANT {priv} ON CATALOG `{catalog_name}` TO `{user_email}`\")\n        \n        spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG `{catalog_name}` TO `{user_email}`\")\n        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": True, \"status\": \"success\"}\n        \n    except Exception as e:\n        print(f\"‚ùå Error granting permissions on {catalog_name} to {user_email}: {str(e)}\")\n        return {\"catalog\": catalog_name, \"user\": user_email, \"granted\": False, \"status\": \"error\", \"error\": str(e)}\n\n# Grant permissions for all users\nprint(\"\\nüîê Assigning CAN MANAGE Permissions:\")\nprint(\"=\" * 80)\n\npermission_results = []\nfor user in user_data:\n    result = grant_catalog_permissions(user['catalog_name'], user['email'])\n    permission_results.append(result)\n    user['permissions_granted'] = result['granted']\n\nprint(\"=\" * 80)\nsuccess_count = sum(1 for r in permission_results if r.get('granted', False))\nprint(f\"üìä Granted permissions to {success_count}/{len(permission_results)} catalogs\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Assign CAN MANAGE Permissions\n\nGrant CAN MANAGE permissions to each user for their respective catalog.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9f32c8-502a-4477-97ae-b2a91a6cb240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## 8. Create Volumes with Consistent Naming\n\nCreate a volume in each catalog with a consistent name across all catalogs."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20a5bf39-e00c-4405-bb79-27a4d8419796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## 9. Load Data to Volumes from Delta Share\n\nCopy data from Delta Share volume to each user's volume using the glob pattern."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bf53c3f-bb11-4584-93ad-043e2c695722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "import fnmatch\n\ndef load_data_to_volume(volume_path, delta_share_catalog, delta_share_schema, delta_share_volume, file_pattern, fallback_source):\n    \"\"\"Copy data from Delta Share volume to user volume using glob pattern.\"\"\"\n    try:\n        volume_fs_path = f\"/Volumes/{volume_path.replace('.', '/')}\"\n        data_location = f\"{volume_fs_path}/data\"\n        dbutils.fs.mkdirs(data_location)\n        \n        # Try Delta Share volume first\n        delta_share_volume_path = f\"/Volumes/{delta_share_catalog}/{delta_share_schema}/{delta_share_volume}\"\n        \n        try:\n            dbutils.fs.ls(delta_share_volume_path)\n            source_path = delta_share_volume_path\n            using_delta_share = True\n        except Exception:\n            source_path = fallback_source\n            using_delta_share = False\n        \n        # List and filter files\n        try:\n            all_files = dbutils.fs.ls(source_path)\n            matching_files = [f.path for f in all_files if fnmatch.fnmatch(f.name, file_pattern)]\n            \n            if not matching_files:\n                return {\"volume\": volume_path, \"data_location\": data_location, \"loaded\": False, \"status\": \"no_files_found\", \"files_copied\": 0}\n            \n            # Copy files\n            files_copied = 0\n            for file_path in matching_files:\n                file_name = file_path.split('/')[-1]\n                dest_path = f\"{data_location}/{file_name}\"\n                try:\n                    dbutils.fs.cp(file_path, dest_path)\n                    files_copied += 1\n                except Exception:\n                    pass\n            \n            return {\n                \"volume\": volume_path,\n                \"data_location\": data_location,\n                \"loaded\": True,\n                \"status\": \"success\",\n                \"files_copied\": files_copied,\n                \"source\": \"delta_share\" if using_delta_share else \"fallback\"\n            }\n            \n        except Exception as e:\n            return {\"volume\": volume_path, \"data_location\": data_location, \"loaded\": False, \"status\": \"error_listing\", \"error\": str(e)}\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading data to {volume_path}: {str(e)}\")\n        return {\"volume\": volume_path, \"data_location\": None, \"loaded\": False, \"status\": \"error\", \"error\": str(e)}\n\n# Load data to all volumes\nprint(\"\\nüì• Loading Data to Volumes:\")\nprint(\"=\" * 80)\n\ndata_load_results = []\nfor user in user_data:\n    if user.get('volume_created', False):\n        result = load_data_to_volume(\n            user['volume_name'],\n            delta_share_catalog,\n            delta_share_schema,\n            delta_share_volume,\n            file_glob_pattern,\n            source_data_path\n        )\n        data_load_results.append(result)\n        user['data_location'] = result['data_location']\n        user['data_loaded'] = result['loaded']\n        user['files_copied'] = result.get('files_copied', 0)\n    else:\n        user['data_location'] = None\n        user['data_loaded'] = False\n        user['files_copied'] = 0\n\nprint(\"=\" * 80)\nsuccess_count = sum(1 for r in data_load_results if r.get('loaded', False))\ntotal_files = sum(r.get('files_copied', 0) for r in data_load_results)\ndelta_share_used = sum(1 for r in data_load_results if r.get('source') == 'delta_share')\n\nprint(f\"üìä Loaded data to {success_count}/{len(data_load_results)} volumes\")\nprint(f\"   Total files copied: {total_files}\")\nif delta_share_used > 0:\n    print(f\"   Delta Share used for {delta_share_used} volumes\")"
  },
  {
   "cell_type": "code",
   "source": "def create_schema_and_volume(catalog_name, schema_name, volume_name):\n    \"\"\"Create schema and volume in a catalog.\"\"\"\n    try:\n        volume_path = f\"{catalog_name}.{schema_name}.{volume_name}\"\n        \n        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog_name}`.`{schema_name}` COMMENT 'Schema for user data and volumes'\")\n        spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`{volume_name}` COMMENT 'User data volume'\")\n        \n        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": True, \"status\": \"success\"}\n        \n    except Exception as e:\n        print(f\"‚ùå Error creating volume in {catalog_name}: {str(e)}\")\n        return {\"catalog\": catalog_name, \"volume_path\": volume_path, \"created\": False, \"status\": \"error\", \"error\": str(e)}\n\n# Create volumes for all users\nprint(\"\\nüíæ Creating User Volumes:\")\nprint(\"=\" * 80)\n\nvolume_results = []\nfor user in user_data:\n    result = create_schema_and_volume(user['catalog_name'], schema_name, volume_name)\n    volume_results.append(result)\n    user['volume_name'] = result['volume_path']\n    user['volume_created'] = result['created']\n\nprint(\"=\" * 80)\nsuccess_count = sum(1 for r in volume_results if r.get('created', False))\nprint(f\"üìä Created {success_count}/{len(volume_results)} volumes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4152983a-d5ab-4e39-9042-e3886b8ca145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Generate Provisioning Report Table\n",
    "\n",
    "Create a comprehensive report table showing all provisioning details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b259d2f7-8fde-45aa-b7ee-b5c02b8900bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "# Define schema for the report\n",
    "report_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), False),\n",
    "    StructField(\"user_alias\", StringType(), False),\n",
    "    StructField(\"catalog_created\", StringType(), False),\n",
    "    StructField(\"permissions_assigned\", BooleanType(), False),\n",
    "    StructField(\"volume_name\", StringType(), True),\n",
    "    StructField(\"volume_data_location\", StringType(), True),\n",
    "    StructField(\"provisioning_status\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Build report rows\n",
    "report_rows = []\n",
    "for idx, user in enumerate(user_data, 1):\n",
    "    # Determine overall provisioning status\n",
    "    if user.get('catalog_created', False) and user.get('permissions_granted', False) and user.get('volume_created', False) and user.get('data_loaded', False):\n",
    "        status = \"‚úÖ Complete\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è Partial\"\n",
    "    \n",
    "    report_rows.append(\n",
    "        Row(\n",
    "            user_id=user['email'],\n",
    "            full_name=user['full_name'],\n",
    "            user_alias=user['alias'],\n",
    "            catalog_created=user['catalog_name'],\n",
    "            permissions_assigned=user.get('permissions_granted', False),\n",
    "            volume_name=user.get('volume_name', None),\n",
    "            volume_data_location=user.get('data_location', None),\n",
    "            provisioning_status=status\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create DataFrame\n",
    "report_df = spark.createDataFrame(report_rows, schema=report_schema)\n",
    "\n",
    "# Display report\n",
    "print(\"\\nüìä Provisioning Report:\")\n",
    "print(\"=\" * 80)\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97d52103-2c27-4b8d-91a3-3288cb1d5ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5894966d-453e-49a0-9c94-d6566463dabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_users = len(user_data)\n",
    "catalogs_created = sum(1 for u in user_data if u.get('catalog_created', False))\n",
    "permissions_granted = sum(1 for u in user_data if u.get('permissions_granted', False))\n",
    "volumes_created = sum(1 for u in user_data if u.get('volume_created', False))\n",
    "data_loaded = sum(1 for u in user_data if u.get('data_loaded', False))\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà WORKSHOP PROVISIONING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Users Processed:        {total_users}\")\n",
    "print(f\"Catalogs Created:             {catalogs_created}/{total_users} ({catalogs_created/total_users*100:.1f}%)\")\n",
    "print(f\"Permissions Granted:          {permissions_granted}/{total_users} ({permissions_granted/total_users*100:.1f}%)\")\n",
    "print(f\"Volumes Created:              {volumes_created}/{total_users} ({volumes_created/total_users*100:.1f}%)\")\n",
    "print(f\"Data Loaded:                  {data_loaded}/{total_users} ({data_loaded/total_users*100:.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Success check\n",
    "if catalogs_created == total_users and permissions_granted == total_users and volumes_created == total_users and data_loaded == total_users:\n",
    "    print(\"\\n‚úÖ SUCCESS: All users provisioned successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Some provisioning steps failed. Review the report above for details.\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84285cd6-60c2-49b7-ae05-02fc90529b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Export Report (Optional)\n",
    "\n",
    "Save the provisioning report to a Delta table for audit purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "237e04be-26ce-4052-9e5f-f75c0c335f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Save report to Delta table and output file\nsave_report = config.get(\"save_report_to_delta\", \"Yes\") == \"Yes\"\nreport_catalog = config.get(\"report_catalog\", \"main\")\nreport_schema_name = config.get(\"report_schema\", \"default\")\nreport_table = config.get(\"report_table\", \"provisioning_reports\")\n\n# Always save report to file for cleanup tracking\nreport_output_file = \"../provisioning_report.json\"\n\ntry:\n    import json\n    \n    report_data = {\n        \"base_catalog_name\": base_catalog_name,\n        \"users\": user_data,\n        \"timestamp\": str(report_df.select(\"provisioning_timestamp\").first()[0]) if \"provisioning_timestamp\" in report_df.columns else None\n    }\n    \n    with open(report_output_file, 'w') as f:\n        json.dump(report_data, f, indent=2)\n    \n    print(f\"\\nüíæ Report saved to: {report_output_file}\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è  Could not save report to file: {str(e)}\")\n\n# Optionally save to Delta table\nif save_report:\n    try:\n        from pyspark.sql.functions import current_timestamp, lit\n        \n        report_df_with_timestamp = report_df \\\n            .withColumn(\"provisioning_timestamp\", current_timestamp()) \\\n            .withColumn(\"base_catalog_name\", lit(base_catalog_name))\n        \n        report_table_path = f\"{report_catalog}.{report_schema_name}.{report_table}\"\n        report_df_with_timestamp.write.format(\"delta\").mode(\"append\").saveAsTable(report_table_path)\n        \n        print(f\"üíæ Report saved to: {report_table_path}\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not save to Delta table: {str(e)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cleanup Section - Delete Provisioned Resources\n\n‚ö†Ô∏è **WARNING**: The following cells will DELETE all catalogs created by this workshop setup.\n\n**Instructions:**\n1. Run the first cell to load the provisioning report\n2. Run the second cell and type \"CONFIRM\" in the widget to delete all resources",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport os\n\n# Try to load the provisioning report\nreport_file_paths = [\n    \"../provisioning_report.json\",\n    \"/Workspace/Repos/dbx-sdp-workshop/provisioning_report.json\",\n    \"provisioning_report.json\"\n]\n\ncleanup_data = None\nreport_file_loaded = None\n\nfor path in report_file_paths:\n    try:\n        with open(path, 'r') as f:\n            cleanup_data = json.load(f)\n            report_file_loaded = path\n            break\n    except FileNotFoundError:\n        continue\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error reading {path}: {str(e)}\")\n        continue\n\nif cleanup_data is None:\n    print(\"‚ùå No provisioning report found!\")\n    print(\"   Run the provisioning cells above first to create resources.\")\n    print(\"   The report file (provisioning_report.json) is required for cleanup.\")\nelse:\n    print(f\"‚úÖ Provisioning report loaded from: {report_file_loaded}\")\n    print(f\"   Base catalog name: {cleanup_data.get('base_catalog_name')}\")\n    print(f\"   Users found: {len(cleanup_data.get('users', []))}\")\n    print(f\"   Timestamp: {cleanup_data.get('timestamp', 'N/A')}\")\n    \n    # Display catalogs to be deleted\n    print(f\"\\nüìã Catalogs that will be deleted:\")\n    print(\"=\" * 80)\n    for idx, user in enumerate(cleanup_data.get('users', []), 1):\n        print(f\"{idx}. {user['catalog_name']}\")\n    print(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create confirmation widget if it doesn't exist\ntry:\n    dbutils.widgets.get(\"delete_confirmation\")\nexcept:\n    dbutils.widgets.text(\"delete_confirmation\", \"\", \"Type CONFIRM to delete all catalogs\")\n\nconfirmation = dbutils.widgets.get(\"delete_confirmation\")\n\nprint(\"‚ö†Ô∏è  WARNING: This will permanently delete all provisioned catalogs and their contents!\")\n\nif confirmation != \"CONFIRM\":\n    print(\"\\n‚ùå Deletion cancelled - confirmation not provided\")\n    print(\"   Type 'CONFIRM' in the text field above and re-run this cell to proceed with deletion\")\nelif cleanup_data is None:\n    print(\"\\n‚ùå No provisioning report found - nothing to delete\")\n    print(\"   Run the cell above to load the provisioning report first.\")\nelse:\n    print(\"\\nüóëÔ∏è  Starting catalog deletion process...\")\n    print(\"=\" * 80)\n    \n    deletion_results = []\n    \n    for user in cleanup_data.get('users', []):\n        catalog_name = user.get('catalog_name')\n        \n        try:\n            # Drop the catalog (CASCADE will delete all contents)\n            spark.sql(f\"DROP CATALOG IF EXISTS `{catalog_name}` CASCADE\")\n            print(f\"   ‚úÖ Deleted catalog: {catalog_name}\")\n            deletion_results.append({\n                \"catalog\": catalog_name,\n                \"deleted\": True,\n                \"status\": \"success\"\n            })\n            \n        except Exception as e:\n            print(f\"   ‚ùå Error deleting catalog {catalog_name}: {str(e)}\")\n            deletion_results.append({\n                \"catalog\": catalog_name,\n                \"deleted\": False,\n                \"status\": \"error\",\n                \"error\": str(e)\n            })\n    \n    print(\"=\" * 80)\n    \n    # Display summary\n    success_count = sum(1 for r in deletion_results if r.get('deleted', False))\n    total_count = len(deletion_results)\n    \n    print(f\"\\nüìä Deletion Summary:\")\n    print(f\"   Catalogs deleted: {success_count}/{total_count}\")\n    \n    if success_count == total_count:\n        print(\"\\n‚úÖ All catalogs deleted successfully!\")\n        \n        # Clean up the report file\n        try:\n            if report_file_loaded and os.path.exists(report_file_loaded):\n                os.remove(report_file_loaded)\n                print(f\"   üóëÔ∏è  Removed report file: {report_file_loaded}\")\n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Could not remove report file: {str(e)}\")\n        \n        # Remove confirmation widget\n        dbutils.widgets.remove(\"delete_confirmation\")\n        print(\"   üóëÔ∏è  Removed confirmation widget\")\n    else:\n        print(\"\\n‚ö†Ô∏è  Some catalogs could not be deleted. Review errors above.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cleanup Section - Delete Provisioned Resources\n\n‚ö†Ô∏è **WARNING**: The following cells will DELETE all catalogs created by this workshop setup.\nOnly run these cells if you want to completely remove all provisioned resources.",
   "metadata": {}
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workshop_setup",
   "widgets": {
    "base_catalog_name": {
     "currentValue": "workshop_catalog",
     "nuid": "cc58a4de-6c11-475f-b18a-4a9378a12b0c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workshop_catalog",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workshop_catalog",
      "label": "Base Catalog Name",
      "name": "base_catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delta_share_file": {
     "currentValue": "config.share",
     "nuid": "1c07f086-95bc-4dc7-a944-0317610cf359",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "config.share",
      "label": "Delta Share Configuration File Path",
      "name": "delta_share_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "config.share",
      "label": "Delta Share Configuration File Path",
      "name": "delta_share_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "3620fa84-451c-41c6-a614-aa9e93b65a69",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name for Volumes",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_data_path": {
     "currentValue": "/databricks-datasets/sample_data",
     "nuid": "a15850f1-ec5a-4b99-b18d-af9d807eed47",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/databricks-datasets/sample_data",
      "label": "Source Data Path to Copy to Volumes",
      "name": "source_data_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/databricks-datasets/sample_data",
      "label": "Source Data Path to Copy to Volumes",
      "name": "source_data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "user_list": {
     "currentValue": "marcin.jimenez@databricks.com",
     "nuid": "e04eab3a-a207-42e1-ace2-8c00475b2e2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated)",
      "name": "user_list",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "marcin.jimenez@databricks.com",
      "label": "User List (comma-separated)",
      "name": "user_list",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "user_data_volume",
     "nuid": "e2f96381-8d4e-4670-b45d-37a702056b4f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "user_data_volume",
      "label": "Volume Name (consistent across all catalogs)",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}